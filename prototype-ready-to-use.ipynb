{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ca0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path configuration\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# add feature extraction paths to sys.path\n",
    "ROOT_PATH = Path(\".\").resolve()\n",
    "FEATURE_PATHS = [\n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Metacognition\",\n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Coreference\", \n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Coherence\",\n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Perplexity\",\n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Stylometric\",\n",
    "    ROOT_PATH / \"Features-Extractions\" / \"Temporal Reasoning\",\n",
    "]\n",
    "\n",
    "for path in FEATURE_PATHS:\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.insert(0, str(path))\n",
    "\n",
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# device setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"gpu: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# paths for lexicons and temporal model\n",
    "LEXICON_PATH = ROOT_PATH / \"Features-Extractions\" / \"Metacognition\" / \"jsons\"\n",
    "TEMPORAL_MODEL_PATH = ROOT_PATH / \"models\" / \"Temporal Reasoning\" /  \"temporal-pairwise-model\" / \"roberta_matres_binary\"\n",
    "OUTPUT_PATH = ROOT_PATH / \"output\"\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_input_data(input_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load data from csv, json, or txt file.\"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    suffix = input_path.suffix.lower()\n",
    "\n",
    "    if suffix == '.csv':\n",
    "        df = pd.read_csv(input_path)\n",
    "    elif suffix == '.json':\n",
    "        df = pd.read_json(input_path)\n",
    "    elif suffix == '.txt':\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            texts = [line.strip() for line in f if line.strip()]\n",
    "        df = pd.DataFrame({'text': texts})\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported file format: {suffix}\")\n",
    "\n",
    "    # ensure id column\n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = [f'doc_{i:05d}' for i in range(len(df))]\n",
    "\n",
    "    # ensure text column\n",
    "    if 'text' not in df.columns:\n",
    "        if 'generation' in df.columns:\n",
    "            df['text'] = df['generation']\n",
    "        else:\n",
    "            raise ValueError(\"no 'text' column found\")\n",
    "\n",
    "    print(f\"loaded {len(df)} documents\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# input your file path here (sanitize quotes)\n",
    "raw_input_path = input(\"enter path to input file (csv/json/txt): \").strip()\n",
    "INPUT_FILE = (\n",
    "    raw_input_path[1:-1]\n",
    "    if raw_input_path.startswith((\"'\", '\"')) and raw_input_path.endswith((\"'\", '\"'))\n",
    "    else raw_input_path\n",
    ")\n",
    "\n",
    "df_input = load_input_data(INPUT_FILE)\n",
    "print(df_input.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy models\n",
    "\n",
    "import spacy\n",
    "\n",
    "def load_spacy():\n",
    "    print(\"loading spacy model...\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_trf\")\n",
    "    except OSError:\n",
    "        print(\"downloading en_core_web_trf...\")\n",
    "        from spacy.cli import download\n",
    "        download(\"en_core_web_trf\")\n",
    "        nlp = spacy.load(\"en_core_web_trf\")\n",
    "    \n",
    "    if DEVICE.type == 'cuda':\n",
    "        try:\n",
    "            spacy.require_gpu()\n",
    "            print(\"spacy gpu enabled\")\n",
    "        except:\n",
    "            pass\n",
    "    print(\"spacy loaded\")\n",
    "    return nlp\n",
    "\n",
    "nlp = load_spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11525efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract stylometric features (base_ prefix)\n",
    "\n",
    "\n",
    "from stylometric_extractor import (\n",
    "    load_nlp_resources, extract_stylometric_features, \n",
    "    impute_missing_features\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"extracting stylometric features...\")\n",
    "\n",
    "# load phoneme resources\n",
    "_, cmu_dict, g2p_model = load_nlp_resources()\n",
    "\n",
    "def extract_base_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract stylometric features with base_ prefix\"\"\"\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    doc_ids = df['id'].tolist()\n",
    "    \n",
    "    all_features = []\n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, batch_size=8), total=len(texts), desc=\"base features\")):\n",
    "        text = texts[i]\n",
    "        feats = extract_stylometric_features(doc, text, cmu_dict, g2p_model)\n",
    "        feats['id'] = doc_ids[i]\n",
    "        \n",
    "        # rename with base_ prefix\n",
    "        renamed = {'id': doc_ids[i]}\n",
    "        for k, v in feats.items():\n",
    "            if k != 'id':\n",
    "                renamed[f'base_{k}'] = v\n",
    "        all_features.append(renamed)\n",
    "        \n",
    "        if DEVICE.type == 'cuda' and (i + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    features_df = pd.DataFrame(all_features)\n",
    "    return features_df\n",
    "\n",
    "base_features_df = extract_base_features(df_input)\n",
    "print(f\"extracted base features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract perplexity features (perp_ prefix)\n",
    "\n",
    "\n",
    "from perplexity_extractor import (\n",
    "    load_language_model, compute_log_perplexity, \n",
    "    compute_token_log_probabilities, create_perturbation\n",
    ")\n",
    "\n",
    "print(\"loading pythia model for perplexity...\")\n",
    "perp_model, perp_tokenizer = load_language_model(DEVICE)\n",
    "\n",
    "def extract_perp_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract perplexity features with perp_ prefix\"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"perp features\"):\n",
    "        doc_id = row['id']\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        features = {'id': doc_id}\n",
    "        \n",
    "        # document perplexity\n",
    "        doc_ppl = compute_log_perplexity(text, perp_model, perp_tokenizer, DEVICE)\n",
    "        features['perp_doc_perplexity'] = doc_ppl\n",
    "        \n",
    "        # sentence-level stats\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        sent_ppls = []\n",
    "        for sent in sentences:\n",
    "            ppl = compute_log_perplexity(sent, perp_model, perp_tokenizer, DEVICE)\n",
    "            if not np.isnan(ppl) and np.isfinite(ppl):\n",
    "                sent_ppls.append(ppl)\n",
    "        \n",
    "        features['perp_mean_sentence_perplexity'] = np.mean(sent_ppls) if sent_ppls else np.nan\n",
    "        features['perp_sentence_perplexity_variance'] = np.var(sent_ppls) if len(sent_ppls) > 1 else np.nan\n",
    "        \n",
    "        # token probability entropy\n",
    "        token_log_probs = compute_token_log_probabilities(text, perp_model, perp_tokenizer, DEVICE)\n",
    "        if len(token_log_probs) > 0:\n",
    "            token_probs = np.exp(token_log_probs)\n",
    "            features['perp_token_probability_entropy'] = -np.sum(token_probs * token_log_probs)\n",
    "        else:\n",
    "            features['perp_token_probability_entropy'] = np.nan\n",
    "        \n",
    "        # curvature\n",
    "        if not np.isnan(doc_ppl) and not np.isnan(features['perp_mean_sentence_perplexity']):\n",
    "            features['perp_curvature'] = doc_ppl - features['perp_mean_sentence_perplexity']\n",
    "        else:\n",
    "            features['perp_curvature'] = np.nan\n",
    "        \n",
    "        # burstiness\n",
    "        if len(sent_ppls) > 1:\n",
    "            valid = [p for p in sent_ppls if np.isfinite(p) and p > 0]\n",
    "            features['perp_burstiness'] = np.max(valid) / np.min(valid) if len(valid) > 1 else np.nan\n",
    "        else:\n",
    "            features['perp_burstiness'] = np.nan\n",
    "        \n",
    "        # trajectory slope\n",
    "        if len(sent_ppls) > 2:\n",
    "            try:\n",
    "                slope, _ = np.polyfit(range(len(sent_ppls)), sent_ppls, 1)\n",
    "                features['perp_trajectory_slope'] = slope\n",
    "            except:\n",
    "                features['perp_trajectory_slope'] = np.nan\n",
    "        else:\n",
    "            features['perp_trajectory_slope'] = np.nan\n",
    "        \n",
    "        # perturbation discrepancy\n",
    "        if not np.isnan(doc_ppl) and len(text.split()) > 20:\n",
    "            pert_ppls = []\n",
    "            for _ in range(5):\n",
    "                pert_text = create_perturbation(text)\n",
    "                pert_ppl = compute_log_perplexity(pert_text, perp_model, perp_tokenizer, DEVICE)\n",
    "                if not np.isnan(pert_ppl):\n",
    "                    pert_ppls.append(pert_ppl)\n",
    "            if len(pert_ppls) >= 2:\n",
    "                mean_pert, std_pert = np.mean(pert_ppls), np.std(pert_ppls)\n",
    "                features['perp_perturbation_discrepancy'] = (doc_ppl - mean_pert) / std_pert if std_pert > 0 else doc_ppl - mean_pert\n",
    "            else:\n",
    "                features['perp_perturbation_discrepancy'] = np.nan\n",
    "        else:\n",
    "            features['perp_perturbation_discrepancy'] = np.nan\n",
    "        \n",
    "        all_features.append(features)\n",
    "        \n",
    "        if DEVICE.type == 'cuda' and (idx + 1) % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "perp_features_df = extract_perp_features(df_input)\n",
    "print(f\"extracted perplexity features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract metacognition features\n",
    "\n",
    "\n",
    "from metacognition_extractor import (\n",
    "    load_lexicons, build_all_lookups, extract_metacognition_features\n",
    ")\n",
    "\n",
    "print(\"loading metacognition lexicons...\")\n",
    "os.chdir(str(ROOT_PATH / \"Features-Extractions\" / \"Metacognition\"))\n",
    "meta_lexicons = load_lexicons()\n",
    "meta_lookups = build_all_lookups(meta_lexicons)\n",
    "os.chdir(str(ROOT_PATH))\n",
    "\n",
    "def extract_meta_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract metacognition features with meta_ prefix\"\"\"\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    doc_ids = df['id'].tolist()\n",
    "    \n",
    "    all_features = []\n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, batch_size=8), total=len(texts), desc=\"meta features\")):\n",
    "        feats = extract_metacognition_features(doc, doc_ids[i], meta_lookups)\n",
    "        \n",
    "        # rename with meta_ prefix\n",
    "        renamed = {'id': doc_ids[i]}\n",
    "        for k, v in feats.items():\n",
    "            if k != 'id':\n",
    "                renamed[f'meta_{k}'] = v\n",
    "        all_features.append(renamed)\n",
    "        \n",
    "        if DEVICE.type == 'cuda' and (i + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "meta_features_df = extract_meta_features(df_input)\n",
    "print(f\"extracted metacognitive features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e636f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract calibration features (calib_ prefix)\n",
    "\n",
    "\n",
    "from calibration_extractor import (\n",
    "    extract_sentence_densities, compute_correlation_feature\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def extract_calib_features(df: pd.DataFrame, meta_df: pd.DataFrame, perp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract calibration features with calib_ prefix\"\"\"\n",
    "    \n",
    "    # build hedge/booster sets from lookups\n",
    "    hedges = meta_lookups.get('hedges', {})\n",
    "    hedge_set = set(hedges.keys()) if isinstance(hedges, dict) else set()\n",
    "    boosters = meta_lookups.get('boosters', {})\n",
    "    booster_set = set(boosters.keys()) if isinstance(boosters, dict) else set()\n",
    "    \n",
    "    all_features = []\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    doc_ids = df['id'].tolist()\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, batch_size=8), total=len(texts), desc=\"calib features\")):\n",
    "        doc_id = doc_ids[i]\n",
    "        features = {'id': doc_id}\n",
    "        \n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        if len(sentences) < 5:\n",
    "            features['calib_hedge_perplexity_correlation'] = 0.0\n",
    "            features['calib_booster_perplexity_anticorrelation'] = 0.0\n",
    "            features['calib_metacog_spike_perplexity_ratio'] = 1.0\n",
    "            features['calib_certainty_perplexity_alignment'] = 0.0\n",
    "            features['calib_reformulation_complexity_match'] = 0.0\n",
    "        else:\n",
    "            # compute per-sentence perplexity and densities\n",
    "            sent_ppls, hedge_dens, booster_dens = [], [], []\n",
    "            for sent in sentences:\n",
    "                ppl = compute_log_perplexity(sent, perp_model, perp_tokenizer, DEVICE)\n",
    "                if not np.isnan(ppl):\n",
    "                    sent_ppls.append(ppl)\n",
    "                    hedge_dens.append(extract_sentence_densities(sent, hedge_set))\n",
    "                    booster_dens.append(extract_sentence_densities(sent, booster_set))\n",
    "            \n",
    "            # correlations\n",
    "            if len(sent_ppls) >= 5 and np.var(hedge_dens) > 0:\n",
    "                try:\n",
    "                    r, _ = pearsonr(hedge_dens, sent_ppls)\n",
    "                    features['calib_hedge_perplexity_correlation'] = 0.0 if np.isnan(r) else r\n",
    "                except:\n",
    "                    features['calib_hedge_perplexity_correlation'] = 0.0\n",
    "            else:\n",
    "                features['calib_hedge_perplexity_correlation'] = 0.0\n",
    "            \n",
    "            if len(sent_ppls) >= 5 and np.var(booster_dens) > 0:\n",
    "                try:\n",
    "                    r, _ = pearsonr(booster_dens, sent_ppls)\n",
    "                    features['calib_booster_perplexity_anticorrelation'] = 0.0 if np.isnan(r) else r\n",
    "                except:\n",
    "                    features['calib_booster_perplexity_anticorrelation'] = 0.0\n",
    "            else:\n",
    "                features['calib_booster_perplexity_anticorrelation'] = 0.0\n",
    "            \n",
    "            # spike ratio\n",
    "            if sent_ppls:\n",
    "                baseline = np.mean(sent_ppls)\n",
    "                features['calib_metacog_spike_perplexity_ratio'] = min(np.max(sent_ppls) / baseline, 2.5) if baseline > 0 else 1.0\n",
    "            else:\n",
    "                features['calib_metacog_spike_perplexity_ratio'] = 1.0\n",
    "            \n",
    "            # certainty-perplexity alignment\n",
    "            meta_row = meta_df[meta_df['id'] == doc_id]\n",
    "            perp_row = perp_df[perp_df['id'] == doc_id]\n",
    "            \n",
    "            if not meta_row.empty and not perp_row.empty:\n",
    "                cert = meta_row['meta_certainty_overall'].values[0] if 'meta_certainty_overall' in meta_row.columns else 0.5\n",
    "                doc_ppl = perp_row['perp_doc_perplexity'].values[0] if 'perp_doc_perplexity' in perp_row.columns else 1.0\n",
    "                if not np.isnan(doc_ppl):\n",
    "                    perp_norm = 1 / (1 + doc_ppl)\n",
    "                    features['calib_certainty_perplexity_alignment'] = 1 - abs(cert - perp_norm)\n",
    "                else:\n",
    "                    features['calib_certainty_perplexity_alignment'] = 0.0\n",
    "            else:\n",
    "                features['calib_certainty_perplexity_alignment'] = 0.0\n",
    "            \n",
    "            # reformulation-complexity match\n",
    "            if not meta_row.empty and not perp_row.empty:\n",
    "                reform = meta_row['meta_reformulation_density'].values[0] if 'meta_reformulation_density' in meta_row.columns else 0.0\n",
    "                sent_var = perp_row['perp_sentence_perplexity_variance'].values[0] if 'perp_sentence_perplexity_variance' in perp_row.columns else 0.0\n",
    "                if not np.isnan(sent_var) and sent_var >= 0.01:\n",
    "                    features['calib_reformulation_complexity_match'] = reform * sent_var\n",
    "                else:\n",
    "                    features['calib_reformulation_complexity_match'] = 0.0\n",
    "            else:\n",
    "                features['calib_reformulation_complexity_match'] = 0.0\n",
    "        \n",
    "        all_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "calib_features_df = extract_calib_features(df_input, meta_features_df, perp_features_df)\n",
    "print(f\"extracted calibration features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract coreference features (coref_ prefix)\n",
    "\n",
    "from fastcoref import FCoref\n",
    "\n",
    "from coreference_extraction_local import (\n",
    "    load_models as load_coref_models, extract_all_features as extract_coref_all\n",
    ")\n",
    "\n",
    "print(\"loading coreference model...\")\n",
    "coref_model, coref_nlp = load_coref_models(str(DEVICE))\n",
    "\n",
    "def extract_coref_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract coreference features with coref_ prefix\"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"coref features\"):\n",
    "        doc_id = row['id']\n",
    "        text = str(row['text'])\n",
    "        \n",
    "        feats = extract_coref_all(text, coref_model, coref_nlp)\n",
    "        \n",
    "        # rename with coref_ prefix\n",
    "        renamed = {'id': doc_id}\n",
    "        for k, v in feats.items():\n",
    "            renamed[f'coref_{k}'] = v\n",
    "        all_features.append(renamed)\n",
    "        \n",
    "        if DEVICE.type == 'cuda' and (idx + 1) % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "coref_features_df = extract_coref_features(df_input)\n",
    "print(f\"extracted {len(coref_features_df.columns) - 1} coref features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract coherence features (coher_ prefix)\n",
    "\n",
    "from coherence_extractor import (\n",
    "    load_sentence_transformer, extract_entities_from_doc,\n",
    "    compute_entity_features, compute_semantic_features,\n",
    "    create_empty_entity_features, create_empty_semantic_features\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"loading sentence-bert...\")\n",
    "sbert = load_sentence_transformer()\n",
    "\n",
    "def extract_coher_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extract coherence features with coher_ prefix\"\"\"\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    doc_ids = df['id'].tolist()\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, batch_size=8), total=len(texts), desc=\"coher features\")):\n",
    "        doc_id = doc_ids[i]\n",
    "        \n",
    "        # entity features\n",
    "        entity_info = extract_entities_from_doc(doc, doc_id)\n",
    "        if len(entity_info['sentences']) >= 2 and len(entity_info['all_entities']) > 0:\n",
    "            entity_feats = compute_entity_features(doc_id, entity_info)\n",
    "        else:\n",
    "            entity_feats = create_empty_entity_features(doc_id)\n",
    "        \n",
    "        # semantic features\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        if len(sentences) >= 2:\n",
    "            embeddings = sbert.encode(sentences, batch_size=32, convert_to_numpy=True, show_progress_bar=False)\n",
    "            semantic_feats = compute_semantic_features(doc_id, embeddings)\n",
    "        else:\n",
    "            semantic_feats = create_empty_semantic_features(doc_id)\n",
    "        \n",
    "        # combine and rename with coher_ prefix\n",
    "        renamed = {'id': doc_id}\n",
    "        for feats in [entity_feats, semantic_feats]:\n",
    "            for k, v in feats.items():\n",
    "                if k != 'id':\n",
    "                    renamed[f'coher_{k}'] = v\n",
    "        \n",
    "        all_features.append(renamed)\n",
    "        \n",
    "        if DEVICE.type == 'cuda' and (i + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "coher_features_df = extract_coher_features(df_input)\n",
    "print(f\"extracted {len(coher_features_df.columns) - 1} coher features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92102f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal features extraction (temp_ prefix)\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure temporal reasoning path is in sys.path\n",
    "TEMPORAL_PATH = ROOT_PATH / \"Features-Extractions\" / \"Temporal Reasoning\"\n",
    "if str(TEMPORAL_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(TEMPORAL_PATH))\n",
    "\n",
    "# import extraction functions from temporal_extraction_local.py\n",
    "from temporal_extraction_local import (\n",
    "    Config as ExtractionConfig,\n",
    "    check_java,\n",
    "    download_corenlp,\n",
    "    start_corenlp_server,\n",
    "    setup_stanza,\n",
    "    load_temporal_classifier,\n",
    "    extract_events_stanza,\n",
    "    extract_temporal_expressions,\n",
    "    extract_binary_relations,\n",
    "    build_raw_temporal_graph,\n",
    "    build_greedy_temporal_graph,\n",
    ")\n",
    "\n",
    "# import feature functions from temporal_features_local.py\n",
    "from temporal_features_local import (\n",
    "    Config as FeatureConfig,\n",
    "    extract_event_structure_features,\n",
    "    extract_relation_features,\n",
    "    extract_graph_features,\n",
    "    extract_constraint_features,\n",
    "    extract_form_meaning_features,\n",
    "    extract_graph_organization_features,\n",
    "    create_empty_features,\n",
    ")\n",
    "\n",
    "\n",
    "# step 0: check and install java if needed\n",
    "print(\"\\n checking java installation\")\n",
    "if not check_java():\n",
    "    print(\"  java not found, attempting installation...\")\n",
    "    try:\n",
    "        subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n",
    "        subprocess.run(['apt-get', 'install', '-y', 'default-jdk'], check=True, capture_output=True)\n",
    "        print(\"  java installed successfully\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"failed to install java: {e}. please install java manually.\")\n",
    "else:\n",
    "    print(\"  java found\")\n",
    "\n",
    "# step 1: setup stanza\n",
    "print(\"\\n initializing stanza...\")\n",
    "nlp_stanza = setup_stanza(DEVICE)\n",
    "\n",
    "# step 2: setup corenlp for sutime\n",
    "print(\"\\n  setting up stanford corenlp...\")\n",
    "CORENLP_PATH = ROOT_PATH / \"corenlp\"\n",
    "CORENLP_PATH.mkdir(exist_ok=True)\n",
    "corenlp_dir = download_corenlp(str(CORENLP_PATH))\n",
    "nlp_corenlp = start_corenlp_server(corenlp_dir)\n",
    "print(f\"  corenlp server started from: {corenlp_dir}\")\n",
    "\n",
    "# step 3: load trained temporal classifier\n",
    "print(\"\\n loading temporal classifier...\")\n",
    "if not TEMPORAL_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"temporal model not found at: {TEMPORAL_MODEL_PATH}\")\n",
    "temporal_model, temporal_tokenizer = load_temporal_classifier(str(TEMPORAL_MODEL_PATH), DEVICE)\n",
    "print(f\"  model loaded from: {TEMPORAL_MODEL_PATH}\")\n",
    "\n",
    "# step 4: extract events, timex, and relations\n",
    "print(\"\\n[  extracting events, temporal expressions, and relations...\")\n",
    "texts_dict = {row['id']: str(row['text']) for _, row in df_input.iterrows()}\n",
    "events_dict = {}\n",
    "timex_dict = {}\n",
    "relations_dict = {}\n",
    "\n",
    "for idx, row in tqdm(df_input.iterrows(), total=len(df_input), desc=\"extraction\"):\n",
    "    doc_id = row['id']\n",
    "    text = str(row['text'])\n",
    "    \n",
    "    try:\n",
    "        # extract events using stanza\n",
    "        events = extract_events_stanza(nlp_stanza, text, doc_id)\n",
    "        events_dict[doc_id] = events\n",
    "        \n",
    "        # extract temporal expressions using sutime\n",
    "        timex = extract_temporal_expressions(nlp_corenlp, text, doc_id)\n",
    "        timex_dict[doc_id] = timex\n",
    "        \n",
    "        # extract binary temporal relations\n",
    "        if len(events) >= 2:\n",
    "            relations = extract_binary_relations(\n",
    "                temporal_model, temporal_tokenizer,\n",
    "                text, events, doc_id, DEVICE\n",
    "            )\n",
    "            relations_dict[doc_id] = relations\n",
    "        else:\n",
    "            relations_dict[doc_id] = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  error {doc_id}: {e}\")\n",
    "        events_dict[doc_id] = []\n",
    "        timex_dict[doc_id] = []\n",
    "        relations_dict[doc_id] = []\n",
    "    \n",
    "    if DEVICE.type == 'cuda' and (idx + 1) % 50 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"  events: {sum(len(e) for e in events_dict.values()):,}\")\n",
    "print(f\"  timex: {sum(len(t) for t in timex_dict.values()):,}\")\n",
    "print(f\"  relations: {sum(len(r) for r in relations_dict.values()):,}\")\n",
    "\n",
    "# step 5: build temporal graphs\n",
    "print(\"\\n  building temporal graphs...\")\n",
    "raw_graphs = {}\n",
    "dag_graphs = {}\n",
    "\n",
    "for doc_id in tqdm(events_dict.keys(), desc=\"graph construction\"):\n",
    "    events = events_dict[doc_id]\n",
    "    relations = relations_dict[doc_id]\n",
    "    \n",
    "    raw_graphs[doc_id] = build_raw_temporal_graph(relations)\n",
    "    dag_graphs[doc_id] = build_greedy_temporal_graph(events, relations, doc_id)\n",
    "\n",
    "non_empty_raw = sum(1 for g in raw_graphs.values() if g is not None)\n",
    "non_empty_dag = sum(1 for g in dag_graphs.values() if g is not None)\n",
    "print(f\"  raw graphs: {non_empty_raw}, dag graphs: {non_empty_dag}\")\n",
    "\n",
    "# step 6: extract all 32 temporal features\n",
    "print(\"\\n  extracting temporal features...\")\n",
    "all_features = []\n",
    "\n",
    "for doc_id in tqdm(events_dict.keys(), desc=\"feature extraction\"):\n",
    "    try:\n",
    "        events = events_dict.get(doc_id, [])\n",
    "        timex_list = timex_dict.get(doc_id, [])\n",
    "        relations = relations_dict.get(doc_id, [])\n",
    "        raw_graph = raw_graphs.get(doc_id)\n",
    "        dag_graph = dag_graphs.get(doc_id)\n",
    "        text = texts_dict.get(doc_id, \"\")\n",
    "        \n",
    "        features = {'id': doc_id}\n",
    "        features.update(extract_event_structure_features(doc_id, events, timex_list, text))\n",
    "        features.update(extract_relation_features(relations, raw_graph))\n",
    "        features.update(extract_graph_features(dag_graph, raw_graph))\n",
    "        features.update(extract_constraint_features(relations, events))\n",
    "        features.update(extract_form_meaning_features(text, events, timex_list, relations, doc_id))\n",
    "        features.update(extract_graph_organization_features(dag_graph))\n",
    "        \n",
    "        all_features.append(features)\n",
    "    except Exception as e:\n",
    "        print(f\"  error {doc_id}: {e}\")\n",
    "        all_features.append(create_empty_features(doc_id))\n",
    "\n",
    "temp_features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# cleanup\n",
    "print(\"\\ncleaning up...\")\n",
    "try:\n",
    "    nlp_corenlp.close()\n",
    "    print(\"  corenlp server closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "del nlp_stanza, temporal_model, temporal_tokenizer\n",
    "del events_dict, timex_dict, relations_dict, raw_graphs, dag_graphs\n",
    "gc.collect()\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"temporal features extracted\")\n",
    "print(f\"\\nfeature columns:\")\n",
    "for col in sorted([c for c in temp_features_df.columns if c != 'id']):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge all features\n",
    "\n",
    "\n",
    "def merge_all_features(*dfs) -> pd.DataFrame:\n",
    "    \"\"\"merge all feature dataframes on id\"\"\"\n",
    "    result = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        result = result.merge(df, on='id', how='outer')\n",
    "    return result\n",
    "\n",
    "# merge all feature sets\n",
    "all_features_df = merge_all_features(\n",
    "    base_features_df,\n",
    "    perp_features_df,\n",
    "    meta_features_df,\n",
    "    calib_features_df,\n",
    "    coref_features_df,\n",
    "    coher_features_df,\n",
    "    temp_features_df\n",
    ")\n",
    "\n",
    "print(f\"\\nmerged dataset shape: {all_features_df.shape}\")\n",
    "print(f\"total features: {len(all_features_df.columns) - 1}\")\n",
    "\n",
    "# feature category breakdown\n",
    "categories = {\n",
    "    'base_': 'stylometric (baseline)',\n",
    "    'perp_': 'perplexity',\n",
    "    'meta_': 'metacognition',\n",
    "    'calib_': 'calibration',\n",
    "    'coref_': 'coreference',\n",
    "    'coher_': 'coherence',\n",
    "    'temp_': 'temporal'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  handle missing values and save output\n",
    "\n",
    "\n",
    "def clean_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"clean and impute missing values\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # replace inf with nan\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # report missing\n",
    "    nan_counts = df.isna().sum()\n",
    "    nan_cols = nan_counts[nan_counts > 0]\n",
    "    if len(nan_cols) > 0:\n",
    "        print(f\"columns with missing values: {len(nan_cols)}\")\n",
    "        for col in nan_cols.index[:10]:\n",
    "            print(f\"  {col}: {nan_cols[col]} ({100*nan_cols[col]/len(df):.1f}%)\")\n",
    "        if len(nan_cols) > 10:\n",
    "            print(f\"  ... and {len(nan_cols) - 10} more\")\n",
    "    \n",
    "    # impute with median\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_cols:\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median()\n",
    "            if np.isnan(median_val):\n",
    "                median_val = 0.0\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# clean features\n",
    "all_features_clean = clean_features(all_features_df)\n",
    "\n",
    "# save to csv\n",
    "output_file = OUTPUT_PATH / \"all_cognitive_features.csv\"\n",
    "all_features_clean.to_csv(output_file, index=False)\n",
    "print(f\"\\nfeatures saved to: {output_file}\")\n",
    "\n",
    "# also save original input with features merged\n",
    "output_merged = OUTPUT_PATH / \"input_with_features.csv\"\n",
    "df_with_features = df_input.merge(all_features_clean, on='id', how='left')\n",
    "df_with_features.to_csv(output_merged, index=False)\n",
    "print(f\"merged data saved to: {output_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  summary statistics\n",
    "\n",
    "\n",
    "print(f\"\\ndocuments processed: {len(all_features_clean)}\")\n",
    "\n",
    "# summary stats for each category\n",
    "print(\"\\nfeature statistics by category:\")\n",
    "for prefix, name in categories.items():\n",
    "    cols = [c for c in all_features_clean.columns if c.startswith(prefix)]\n",
    "    if cols:\n",
    "        subset = all_features_clean[cols]\n",
    "        print(f\"\\n{name} ({len(cols)} features):\")\n",
    "        print(f\"  mean range: [{subset.mean().min():.4f}, {subset.mean().max():.4f}]\")\n",
    "        print(f\"  nan count: {subset.isna().sum().sum()}\")\n",
    "\n",
    "# sample output\n",
    "print(\"\\nsample output (first 3 rows, first 10 columns):\")\n",
    "print(all_features_clean.iloc[:3, :10].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
