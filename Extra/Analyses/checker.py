# -*- coding: utf-8 -*-
"""checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CN4UdqbBFXfF6pZHOGXuAnJhBzueElP
"""

# Analysis recap for feature group experiments

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report)
from xgboost import XGBClassifier
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

print("Mounting drive and setting up paths")

from google.colab import drive
drive.mount('/content/drive')

# Path configuration
BASE_DRIVE = '/content/drive/MyDrive/Tesi Magistrale'
FEATURES_DIR = '/content'

MASTER_DATA_PATH = f'{BASE_DRIVE}/master_features/master_features_complete_2.csv'
OUTPUT_DIR = f'{BASE_DRIVE}/analysis_results'

# Create output directories
import os
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(f'{OUTPUT_DIR}/models', exist_ok=True)
os.makedirs(f'{OUTPUT_DIR}/figures', exist_ok=True)
os.makedirs(f'{OUTPUT_DIR}/reports', exist_ok=True)

# Define your feature group files
FEATURE_GROUP_FILES = {
    'BASELINE': 'baseline_features.csv',
    'BACKBONE': 'backbone_features.csv',
    'COREFERENCE': 'coreference_features.csv',
    'COHESION': 'cohesion_features.csv',
    'PERPLEXITY': 'perplexity_features.csv',
    'TEMPORAL': 'temporal_features.csv',
    'METACOGNITION': 'metacognition_features.csv',
    'CALIBRATION': 'calibration_features.csv'
}

# Load feature groups
FEATURE_CATEGORIES = {}
feature_loading_summary = []

for group_name, filename in FEATURE_GROUP_FILES.items():
    filepath = f'{FEATURES_DIR}/{filename}'

    try:
        df_group = pd.read_csv(filepath)

        # Extract feature names
        if 'feature' in df_group.columns:
            features = df_group['feature'].tolist()
        else:
            features = df_group.iloc[:, 0].tolist()

        # Clean feature names (remove whitespace, NaN)
        features = [str(f).strip() for f in features if pd.notna(f)]

        FEATURE_CATEGORIES[group_name] = features

        print(f"  {group_name:15s}: {len(features):4d} features loaded")

    except FileNotFoundError:
        print(f"  {group_name:15s}: File not found - {filepath}")
        FEATURE_CATEGORIES[group_name] = []
        feature_loading_summary.append({
            'group': group_name,
            'file': filename,
            'n_features': 0,
            'status': 'FILE NOT FOUND'
        })
    except Exception as e:
        print(f"  {group_name:15s}: Error - {str(e)}")
        FEATURE_CATEGORIES[group_name] = []
        feature_loading_summary.append({
            'group': group_name,
            'file': filename,
            'n_features': 0,
            'status': f'ERROR: {str(e)}'
        })

# Summary
df_loading_summary = pd.DataFrame(feature_loading_summary)

# Load master dataframe
df_master = pd.read_csv(MASTER_DATA_PATH)


validated_categories = {}
validation_summary = []

for group_name, features in FEATURE_CATEGORIES.items():
    valid_features = [f for f in features if f in df_master.columns]
    missing_features = [f for f in features if f not in df_master.columns]

    validated_categories[group_name] = valid_features

    validation_summary.append({
        'group': group_name,
        'missing': len(missing_features),
        'coverage': len(valid_features) / len(features) * 100 if len(features) > 0 else 0
    })


# Update FEATURE_CATEGORIES with validated features
FEATURE_CATEGORIES = validated_categories

df_validation = pd.DataFrame(validation_summary)

print("Validation Summary:")
print(df_validation.to_string(index=False))

X = df_master.drop(columns=['id', 'is_ai'])
y = df_master['is_ai']

print(f"Class balance: AI={y.mean()}, Human={(~y.astype(bool)).mean()}")

EXPERIMENTS = {}

#Individual cognitive feature groups
for group_name, features in FEATURE_CATEGORIES.items():
    if group_name == 'BACKBONE':
        continue

    if len(features) > 0:
        EXPERIMENTS[f'group_{group_name.lower()}'] = {
            'name': f'GROUP: {group_name}',
            'features': features,
            'description': f'Only {group_name} features'
        }

#All features combined
all_features = [f for group_features in FEATURE_CATEGORIES.values()
                for f in group_features]
all_features = list(dict.fromkeys(all_features))

EXPERIMENTS['all_features'] = {
    'name': 'ALL FEATURES',
    'features': all_features,
    'description': 'All feature groups combined'
}

# 3.All cognitive
cognitive_groups = [g for g in FEATURE_CATEGORIES.keys()
                    if g not in ['BASELINE', 'BACKBONE']]
cognitive_features = [f for g in cognitive_groups
                      for f in FEATURE_CATEGORIES[g]]
cognitive_features = list(dict.fromkeys(cognitive_features))

if len(cognitive_features) > 0:
    EXPERIMENTS['all_cognitive'] = {
        'name': 'ALL COGNITIVE',
        'features': cognitive_features,
        'description': 'All cognitive features'
    }


# Validate all experiments
for exp_name, exp_config in EXPERIMENTS.items():
    features = exp_config['features']
    valid_features = [f for f in features if f in X.columns]  # ADD THIS LINE

    exp_config['valid_features'] = valid_features
    exp_config['n_features'] = len(valid_features)

# Group experiments by type
exp_types = {
    'Individual Groups': [k for k in EXPERIMENTS.keys() if k.startswith('group_')],
    'Combined': [k for k in EXPERIMENTS.keys() if k in ['all_features', 'all_cognitive']],
}

for exp_type, exp_names in exp_types.items():
    if len(exp_names) > 0:
        for exp_name in exp_names:
            config = EXPERIMENTS[exp_name]

total_experiments = sum(len(v) for v in exp_types.values())
print(f"Total experiments: {total_experiments}")

# XGBOOST CONFIGURATION

XGBOOST_PARAMS = {
    'n_estimators': 500,
    'max_depth': 9,
    'learning_rate': 0.15,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'n_jobs': -1,
    'eval_metric': 'logloss',
    'tree_method': 'hist'  # Added to avoid quantile issues
}

print("XGBoost parameters:")
for param, value in XGBOOST_PARAMS.items():
    print(f"  {param:20s}: {value}")

CV_FOLDS = 5

print(f"Cross-validation: {CV_FOLDS}-fold StratifiedKFold")

# EVALUATION FUNCTION (FIXED)


def evaluate_model(X_exp, y_exp, experiment_name, n_features, verbose=True):

    if verbose:
        print(f"Evaluating {experiment_name}...")
        print(f"  Features: {n_features}")

    # Convert to numpy arrays to avoid DataFrame issues with XGBoost
    X_array = X_exp.values if isinstance(X_exp, pd.DataFrame) else X_exp
    y_array = y_exp.values if isinstance(y_exp, pd.Series) else y_exp

    model = XGBClassifier(**XGBOOST_PARAMS)
    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)

    scoring = {
        'accuracy': 'accuracy',
        'precision': 'precision',
        'recall': 'recall',
        'f1': 'f1',
        'roc_auc': 'roc_auc'
    }

    cv_results = cross_validate(
        model, X_array, y_array,
        cv=cv,
        scoring=scoring,
        return_train_score=True,
        n_jobs=-1
    )

    model.fit(X_array, y_array)
    y_pred = model.predict(X_array)
    y_pred_proba = model.predict_proba(X_array)[:, 1]

    results = {
        'experiment': experiment_name,
        'n_features': n_features,
        'cv_accuracy_mean': cv_results['test_accuracy'].mean(),
        'cv_accuracy_std': cv_results['test_accuracy'].std(),
        'cv_precision_mean': cv_results['test_precision'].mean(),
        'cv_precision_std': cv_results['test_precision'].std(),
        'cv_recall_mean': cv_results['test_recall'].mean(),
        'cv_recall_std': cv_results['test_recall'].std(),
        'cv_f1_mean': cv_results['test_f1'].mean(),
        'cv_f1_std': cv_results['test_f1'].std(),
        'cv_roc_auc_mean': cv_results['test_roc_auc'].mean(),
        'cv_roc_auc_std': cv_results['test_roc_auc'].std(),
        'full_accuracy': accuracy_score(y_array, y_pred),
        'full_precision': precision_score(y_array, y_pred),
        'full_recall': recall_score(y_array, y_pred),
        'full_f1': f1_score(y_array, y_pred),
        'full_roc_auc': roc_auc_score(y_array, y_pred_proba),
        'model': model,
        'cv_results': cv_results,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

    if verbose:
        print(f"  CV F1: {results['cv_f1_mean']:.4f} +/- {results['cv_f1_std']:.4f}")
        print(f"  CV Accuracy: {results['cv_accuracy_mean']:.4f} +/- {results['cv_accuracy_std']:.4f}")
        print(f"  CV ROC-AUC: {results['cv_roc_auc_mean']:.4f} +/- {results['cv_roc_auc_std']:.4f}")

    return results


# RUN ALL EXPERIMENTS

all_results = {}
failed_experiments = []

for exp_name, exp_config in EXPERIMENTS.items():
    print(f"\n{exp_config['name']}")

    features = exp_config['valid_features']

    if len(features) == 0:
        print("No valid features - skipping")
        failed_experiments.append(exp_name)
        continue

    try:
        X_exp = X[features].copy()

        # Handle NaN values
        if X_exp.isna().any().any():
            n_nan = X_exp.isna().sum().sum()
            print(f"{n_nan} NaN values found - filling with median")
            X_exp = X_exp.fillna(X_exp.median())

        # Check for infinite values
        if np.isinf(X_exp.values).any():
            print(f"Infinite values found - replacing with max/min")
            X_exp = X_exp.replace([np.inf, -np.inf], np.nan)
            X_exp = X_exp.fillna(X_exp.median())

        results = evaluate_model(X_exp, y, exp_config['name'], len(features), verbose=True)
        all_results[exp_name] = results

    except Exception as e:
        print(f"ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        failed_experiments.append(exp_name)

print("\n" + "-" * 80)
print(f"Experiments complete: {len(all_results)}/{len(EXPERIMENTS)} successful")
if len(failed_experiments) > 0:
    print(f"Failed experiments: {len(failed_experiments)}")
    for exp in failed_experiments:
        print(f"  - {exp}")