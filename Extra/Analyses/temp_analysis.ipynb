{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_feature_list(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    features = (\n",
        "        df[\"feature\"]\n",
        "        .dropna()\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .unique()\n",
        "        .tolist()\n",
        "    )\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "XdEazaPdo_Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoNpnr-Gss3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9d01a9-eee8-4b83-a599-b8b87cb128e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            " Google Drive mounted successfully\n",
            " Drive path verified\n",
            " Available space on Drive: 208.7 GB\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Temp Analysis\n",
        "\n",
        "# Install required packages\n",
        "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn scipy shap -q\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\" Google Drive mounted successfully\")\n",
        "\n",
        "    # Verify mount\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\" Drive path verified\")\n",
        "    else:\n",
        "        raise Exception(\"Drive mounted but MyDrive not found\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR\")\n",
        "    raise\n",
        "\n",
        "# Check available space\n",
        "import shutil\n",
        "try:\n",
        "    available_space = shutil.disk_usage('/content/drive/MyDrive').free / 1e9\n",
        "    print(f\" Available space on Drive: {available_space:.1f} GB\")\n",
        "    if available_space < 1.0:\n",
        "        print(f\"   WARNING: Low disk space!\")\n",
        "except:\n",
        "    print(f\"   Could not check disk space\")\n",
        "\n",
        "\n",
        "# Install kaleido for static image export\n",
        "!pip install -U kaleido -q\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths - UPDATE THESE to your actual paths\n",
        "ORIGINAL_DF_PATH = '/content/raid_sample_medium_PostPOS_CLEAN (1).csv'\n",
        "TEMPORAL_FEATURES_PATH = '/content/drive/MyDrive/Tesi Magistrale/temporal_features/temporal_features_final.csv'\n",
        "Backbone = '/content/baseline_features.csv'\n",
        "\n",
        "# Output directory for results\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Tesi Magistrale/temporal_analysis'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'plots'), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'tables'), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'checkpoints'), exist_ok=True)\n",
        "\n",
        "df_original = pd.read_csv(ORIGINAL_DF_PATH)\n",
        "df_temporal = pd.read_csv(TEMPORAL_FEATURES_PATH)\n",
        "backbone_features = load_feature_list(backbone)\n",
        "\n",
        "# Identify temporal feature columns\n",
        "temporal_features = [c for c in df_temporal.columns if c != 'id']\n",
        "print(f\" Temporal feature columns: {len(temporal_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWRBIlSts3mW",
        "outputId": "aa03b4b4-b9b7-47dd-cd95-077cfb4dc8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Temporal feature columns: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MERGE\n",
        "\n",
        "\n",
        "# Group temporal features by category\n",
        "print(\"Temporal feature breakdown by category:\")\n",
        "\n",
        "# Category I: Event Structure\n",
        "event_structure_features = [\n",
        "    'temp_num_events',\n",
        "    'temp_events_per_sentence',\n",
        "    'temp_event_lexical_diversity',\n",
        "    'temp_tense_distribution_entropy',\n",
        "    'temp_num_timex',\n",
        "    'temp_timex_event_ratio'\n",
        "]\n",
        "\n",
        "# Category II: Relations\n",
        "relation_features = [\n",
        "    'temp_rel_mean_confidence',\n",
        "    'temp_rel_confidence_variance',\n",
        "    'temp_rel_before_after_ratio',\n",
        "    'temp_rel_raw_cycle_count',\n",
        "    'temp_rel_cycle_edge_ratio',\n",
        "    'temp_rel_parallel_edge_rate',\n",
        "    'temp_rel_type_entropy',\n",
        "    'temp_rel_transitivity_violation_rate',\n",
        "    'temp_rel_cycle_approx_flag'\n",
        "]\n",
        "\n",
        "# Category III: Graph-Theoretic\n",
        "graph_features = [\n",
        "    'tg_edge_retention',\n",
        "    'tg_degree_entropy',\n",
        "    'tg_avg_in_degree',\n",
        "    'tg_avg_out_degree',\n",
        "    'tg_ordering_entropy',\n",
        "    'tg_longest_path',\n",
        "    'tg_mean_depth',\n",
        "    'tg_branching_factor',\n",
        "    'tg_global_coherence'\n",
        "]\n",
        "\n",
        "# Category IV: Constraints\n",
        "constraint_features = [\n",
        "    'temp_constraint_violation_rate',\n",
        "    'temp_constraint_csp_score',\n",
        "    'temp_scope_variance'\n",
        "]\n",
        "\n",
        "# Category V: Form-Meaning\n",
        "form_meaning_features = [\n",
        "    'temp_tense_time_alignment',\n",
        "    'temp_deixis_consistency',\n",
        "    'temp_ref_time_shifts'\n",
        "]\n",
        "\n",
        "# Category VI: Graph Organization\n",
        "graph_org_features = [\n",
        "    'tg_centralization',\n",
        "    'tg_clustering_coefficient',\n",
        "    'tg_density'\n",
        "]\n",
        "\n",
        "# Verify all features are present\n",
        "all_temporal_features = (event_structure_features + relation_features +\n",
        "                         graph_features + constraint_features +\n",
        "                         form_meaning_features + graph_org_features)\n",
        "\n",
        "df_backbone = df_original[['id', 'is_ai'] + backbone_features].copy()\n",
        "df_merged = df_backbone.merge(df_temporal, on='id', how='inner')\n",
        "\n",
        "# Filter to only features that exist in dataframe\n",
        "temporal_features_present = [f for f in all_temporal_features if f in df_merged.columns]\n",
        "missing_temporal = [f for f in all_temporal_features if f not in df_merged.columns]\n",
        "\n",
        "if missing_temporal:\n",
        "    print(f\" WARNING: Missing temporal features: {missing_temporal}\")\n",
        "\n",
        "print(f\"Event Structure: {len([f for f in event_structure_features if f in df_merged.columns])}\")\n",
        "print(f\"Relations: {len([f for f in relation_features if f in df_merged.columns])}\")\n",
        "print(f\"Graph-Theoretic: {len([f for f in graph_features if f in df_merged.columns])}\")\n",
        "print(f\"Constraints: {len([f for f in constraint_features if f in df_merged.columns])}\")\n",
        "print(f\"Form-Meaning: {len([f for f in form_meaning_features if f in df_merged.columns])}\")\n",
        "print(f\"Graph Organization: {len([f for f in graph_org_features if f in df_merged.columns])}\")\n",
        "\n",
        "# Use only present features\n",
        "temporal_features = temporal_features_present\n",
        "\n",
        "# Create feature group mapping\n",
        "feature_groups = {}\n",
        "for feat in event_structure_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Event Structure'\n",
        "for feat in relation_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Relations'\n",
        "for feat in graph_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Graph-Theoretic'\n",
        "for feat in constraint_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Constraints'\n",
        "for feat in form_meaning_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Form-Meaning'\n",
        "for feat in graph_org_features:\n",
        "    if feat in df_merged.columns:\n",
        "        feature_groups[feat] = 'Graph Organization'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sX2GG_ds-a4",
        "outputId": "5d160dcf-89ea-4929-e2da-0f9289ab5d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporal feature breakdown by category:\n",
            "Event Structure: 6\n",
            "Relations: 9\n",
            "Graph-Theoretic: 9\n",
            "Constraints: 3\n",
            "Form-Meaning: 3\n",
            "Graph Organization: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA QUALITY\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Value Analysis:\")\n",
        "nan_counts = df_merged[temporal_features].isna().sum()\n",
        "nan_features = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(nan_features) > 0:\n",
        "    print(f\"  Found {len(nan_features)} features with missing values:\")\n",
        "    for feat, count in nan_features.items():\n",
        "        pct = 100 * count / len(df_merged)\n",
        "        print(f\"  {feat}: {count} ({pct:.2f}%)\")\n",
        "else:\n",
        "    print(\"   No missing values found!\")\n",
        "\n",
        "# Check for infinite values\n",
        "print(\"Infinite Value Check:\")\n",
        "inf_counts = np.isinf(df_merged[temporal_features]).sum()\n",
        "inf_features = inf_counts[inf_counts > 0]\n",
        "\n",
        "if len(inf_features) > 0:\n",
        "    print(f\"  Found {len(inf_features)} features with infinite values:\")\n",
        "    for feat, count in inf_features.items():\n",
        "        print(f\"  {feat}: {count}\")\n",
        "else:\n",
        "    print(\"   No infinite values found!\")\n",
        "\n",
        "# Check for zero-variance features\n",
        "print(\"Zero-Variance Check:\")\n",
        "zero_var_features = [f for f in temporal_features if df_merged[f].std() == 0]\n",
        "\n",
        "if zero_var_features:\n",
        "    print(f\"  Found {len(zero_var_features)} zero-variance features:\")\n",
        "    for feat in zero_var_features:\n",
        "        print(f\"  {feat}: constant value = {df_merged[feat].iloc[0]}\")\n",
        "else:\n",
        "    print(\"   No zero-variance features!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqnFXHTttDIK",
        "outputId": "4c0dadce-3cb1-477e-83a6-c4640723dbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Value Analysis:\n",
            "   No missing values found!\n",
            "Infinite Value Check:\n",
            "   No infinite values found!\n",
            "Zero-Variance Check:\n",
            "   No zero-variance features!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HANDLE INVALID VALUES\n",
        "\n",
        "print(\"HANDLING INVALID VALUES\")\n",
        "\n",
        "df_clean = df_merged.copy()\n",
        "\n",
        "invalid_replacements = 0\n",
        "\n",
        "for feat in temporal_features:\n",
        "    # Count invalids\n",
        "    n_nan = df_clean[feat].isna().sum()\n",
        "    n_inf = np.isinf(df_clean[feat]).sum()\n",
        "\n",
        "    if n_nan > 0 or n_inf > 0:\n",
        "        print(f\"{feat}: {n_nan} NaN, {n_inf} Inf → replacing with 0\")\n",
        "        df_clean[feat] = df_clean[feat].replace([np.inf, -np.inf], np.nan)\n",
        "        df_clean[feat].fillna(0, inplace=True)\n",
        "        invalid_replacements += 1\n",
        "\n",
        "if invalid_replacements == 0:\n",
        "    print(\"   No invalid values to replace!\")\n",
        "else:\n",
        "    print(f\"\\n Replaced invalid values in {invalid_replacements} features\")\n",
        "\n",
        "# Verify no invalids remain\n",
        "remaining_nans = df_clean[temporal_features].isna().sum().sum()\n",
        "remaining_infs = np.isinf(df_clean[temporal_features]).sum().sum()\n",
        "\n",
        "print(f\"Verification:\")\n",
        "print(f\"Remaining NaN: {remaining_nans}\")\n",
        "print(f\"Remaining Inf: {remaining_infs}\")\n",
        "\n",
        "if remaining_nans == 0 and remaining_infs == 0:\n",
        "    print(\"   All invalid values handled!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVi6V0KVtGoA",
        "outputId": "f6a7dbbc-ade2-4109-85e1-b7ef2ee3768f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HANDLING INVALID VALUES\n",
            "   No invalid values to replace!\n",
            "Verification:\n",
            "Remaining NaN: 0\n",
            "Remaining Inf: 0\n",
            "   All invalid values handled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "# Separate AI and Human samples\n",
        "df_ai = df_clean[df_clean['is_ai'] == 1]\n",
        "df_human = df_clean[df_clean['is_ai'] == 0]\n",
        "\n",
        "descriptive_stats = []\n",
        "\n",
        "for feature in temporal_features:\n",
        "    ai_values = df_ai[feature]\n",
        "    human_values = df_human[feature]\n",
        "\n",
        "    descriptive_stats.append({\n",
        "        'feature': feature,\n",
        "        'category': feature_groups.get(feature, 'Unknown'),\n",
        "        'ai_mean': ai_values.mean(),\n",
        "        'ai_std': ai_values.std(),\n",
        "        'ai_median': ai_values.median(),\n",
        "        'human_mean': human_values.mean(),\n",
        "        'human_std': human_values.std(),\n",
        "        'human_median': human_values.median(),\n",
        "        'diff_mean': ai_values.mean() - human_values.mean(),\n",
        "        'diff_pct': 100 * (ai_values.mean() - human_values.mean()) / human_values.mean()\n",
        "                    if human_values.mean() != 0 else np.nan\n",
        "    })\n",
        "\n",
        "df_stats = pd.DataFrame(descriptive_stats)\n",
        "\n",
        "# Save descriptive statistics\n",
        "stats_path = os.path.join(OUTPUT_DIR, 'tables', 'temporal_descriptive_statistics.csv')\n",
        "df_stats.to_csv(stats_path, index=False)\n",
        "print(f\" Descriptive statistics saved: {stats_path}\")\n",
        "\n",
        "# Display top differences by absolute percentage\n",
        "print(\"Top 10 temporal features by relative difference (%):\")\n",
        "df_stats_sorted = df_stats.sort_values('diff_pct', key=abs, ascending=False)\n",
        "print(df_stats_sorted[['feature', 'category', 'ai_mean', 'human_mean', 'diff_pct']].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z9miKNitI4p",
        "outputId": "052cdcfa-f600-445b-bfce-d57e587298db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Descriptive statistics saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/temporal_descriptive_statistics.csv\n",
            "Top 10 temporal features by relative difference (%):\n",
            "                             feature        category      ai_mean    human_mean   diff_pct\n",
            "          temp_rel_cycle_approx_flag       Relations     0.001667      0.033167 -94.974874\n",
            "                 temp_scope_variance     Constraints 50028.699621 287561.246549 -82.602419\n",
            "      temp_constraint_violation_rate     Constraints     0.071064      0.050748  40.032689\n",
            "             temp_deixis_consistency    Form-Meaning     1.120333      1.611833 -30.493227\n",
            "           temp_constraint_csp_score     Constraints    88.011833    113.499833 -22.456421\n",
            "                     temp_num_events Event Structure    27.144667     34.946833 -22.325819\n",
            "                temp_ref_time_shifts    Form-Meaning     0.407833      0.519000 -21.419396\n",
            "temp_rel_transitivity_violation_rate       Relations     1.859204      2.263812 -17.872877\n",
            "            temp_events_per_sentence Event Structure     3.303081      2.807386  17.656804\n",
            "                      temp_num_timex Event Structure     5.577500      6.717000 -16.964419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STATISTICAL TESTING\n",
        "\n",
        "def cohens_d(group1, group2):\n",
        "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1, var2 = group1.var(), group2.var()\n",
        "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
        "    return (group1.mean() - group2.mean()) / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "print(\"Running statistical tests for all temporal features\")\n",
        "\n",
        "statistical_results = []\n",
        "\n",
        "for feature in tqdm(temporal_features, desc=\"Testing features\"):\n",
        "    ai_vals = df_ai[feature].dropna()\n",
        "    human_vals = df_human[feature].dropna()\n",
        "\n",
        "    # T-test\n",
        "    t_stat, p_value = ttest_ind(ai_vals, human_vals, equal_var=False)\n",
        "\n",
        "    # Cohen's d\n",
        "    d = cohens_d(ai_vals, human_vals)\n",
        "\n",
        "    # Effect size interpretation\n",
        "    if abs(d) < 0.2:\n",
        "        effect_size = 'negligible'\n",
        "    elif abs(d) < 0.5:\n",
        "        effect_size = 'small'\n",
        "    elif abs(d) < 0.8:\n",
        "        effect_size = 'medium'\n",
        "    else:\n",
        "        effect_size = 'large'\n",
        "\n",
        "    statistical_results.append({\n",
        "        'feature': feature,\n",
        "        'category': feature_groups.get(feature, 'Unknown'),\n",
        "        't_statistic': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'cohens_d': d,\n",
        "        'effect_size': effect_size,\n",
        "        'significant': p_value < 0.05,\n",
        "        'ai_mean': ai_vals.mean(),\n",
        "        'human_mean': human_vals.mean()\n",
        "    })\n",
        "\n",
        "df_stats_tests = pd.DataFrame(statistical_results)\n",
        "\n",
        "# Apply Bonferroni correction\n",
        "bonferroni_threshold = 0.05 / len(temporal_features)\n",
        "df_stats_tests['bonferroni_significant'] = df_stats_tests['p_value'] < bonferroni_threshold\n",
        "\n",
        "# Save statistical results\n",
        "stats_test_path = os.path.join(OUTPUT_DIR, 'tables', 'temporal_statistical_tests.csv')\n",
        "df_stats_tests.to_csv(stats_test_path, index=False)\n",
        "print(f\"\\n Statistical test results saved: {stats_test_path}\")\n",
        "\n",
        "# Summary statistics\n",
        "n_significant = (df_stats_tests['p_value'] < 0.05).sum()\n",
        "n_bonferroni = df_stats_tests['bonferroni_significant'].sum()\n",
        "\n",
        "print(f\"Statistical significance summary:\")\n",
        "print(f\"Significant (p < 0.05): {n_significant}/{len(temporal_features)} ({100*n_significant/len(temporal_features):.1f}%)\")\n",
        "print(f\"Bonferroni-corrected (p < {bonferroni_threshold:.6f}): {n_bonferroni}/{len(temporal_features)} ({100*n_bonferroni/len(temporal_features):.1f}%)\")\n",
        "\n",
        "# Effect size distribution\n",
        "print(f\"Effect size distribution:\")\n",
        "for effect in ['negligible', 'small', 'medium', 'large']:\n",
        "    count = (df_stats_tests['effect_size'] == effect).sum()\n",
        "    print(f\"{effect.capitalize()}: {count} ({100*count/len(df_stats_tests):.1f}%)\")\n",
        "\n",
        "# Category-level summary\n",
        "print(f\"Significance by category:\")\n",
        "category_summary = df_stats_tests.groupby('category').agg({\n",
        "    'bonferroni_significant': 'sum',\n",
        "    'cohens_d': lambda x: x.abs().mean()\n",
        "}).round(4)\n",
        "category_summary.columns = ['Bonferroni Significant', 'Mean |Cohen\\'s d|']\n",
        "print(category_summary.to_string())\n",
        "\n",
        "# Top features by effect size\n",
        "print(\"Top 15 temporal features by |Cohen's d|:\")\n",
        "df_top_effects = df_stats_tests.sort_values('cohens_d', key=abs, ascending=False).head(15)\n",
        "print(df_top_effects[['feature', 'category', 'cohens_d', 'p_value', 'effect_size']].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYb_HpttMzh",
        "outputId": "765bc2c6-ca31-4695-838d-414d956fc835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running statistical tests for all temporal features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing features: 100%|██████████| 33/33 [00:00<00:00, 281.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Statistical test results saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/temporal_statistical_tests.csv\n",
            "Statistical significance summary:\n",
            "Significant (p < 0.05): 26/33 (78.8%)\n",
            "Bonferroni-corrected (p < 0.001515): 25/33 (75.8%)\n",
            "Effect size distribution:\n",
            "Negligible: 28 (84.8%)\n",
            "Small: 5 (15.2%)\n",
            "Medium: 0 (0.0%)\n",
            "Large: 0 (0.0%)\n",
            "Significance by category:\n",
            "                    Bonferroni Significant  Mean |Cohen's d|\n",
            "category                                                    \n",
            "Constraints                              3            0.1589\n",
            "Event Structure                          4            0.1050\n",
            "Form-Meaning                             3            0.1109\n",
            "Graph Organization                       1            0.0532\n",
            "Graph-Theoretic                          8            0.0994\n",
            "Relations                                6            0.1121\n",
            "Top 15 temporal features by |Cohen's d|:\n",
            "                             feature           category  cohens_d      p_value effect_size\n",
            "          temp_rel_cycle_approx_flag          Relations -0.242537 9.188532e-40       small\n",
            "           temp_constraint_csp_score        Constraints -0.234718 1.446876e-37       small\n",
            "temp_rel_transitivity_violation_rate          Relations -0.214797 1.055138e-31       small\n",
            "                     temp_num_events    Event Structure -0.209321 3.529732e-30       small\n",
            "                     tg_longest_path    Graph-Theoretic -0.203535 1.029322e-28       small\n",
            "           temp_rel_cycle_edge_ratio          Relations -0.194294 2.492556e-26  negligible\n",
            "             temp_deixis_consistency       Form-Meaning -0.175307 1.003186e-21  negligible\n",
            "      temp_constraint_violation_rate        Constraints  0.172542 4.331602e-21  negligible\n",
            "                 tg_ordering_entropy    Graph-Theoretic -0.149361 3.101916e-16  negligible\n",
            "                 tg_global_coherence    Graph-Theoretic  0.144953 2.212943e-15  negligible\n",
            "                      temp_num_timex    Event Structure -0.143045 5.098450e-15  negligible\n",
            "            temp_rel_raw_cycle_count          Relations -0.139466 2.358982e-14  negligible\n",
            "                          tg_density Graph Organization  0.135705 1.135479e-13  negligible\n",
            "     temp_tense_distribution_entropy    Event Structure -0.124363 1.011324e-11  negligible\n",
            "            temp_events_per_sentence    Event Structure  0.089965 8.464082e-07  negligible\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Category-Level Effect Sizes\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "category_colors = {\n",
        "    'Event Structure': '#1f77b4',\n",
        "    'Relations': '#ff7f0e',\n",
        "    'Graph-Theoretic': '#2ca02c',\n",
        "    'Constraints': '#d62728',\n",
        "    'Form-Meaning': '#9467bd',\n",
        "    'Graph Organization': '#8c564b'\n",
        "}\n",
        "\n",
        "for idx, (category, color) in enumerate(category_colors.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    cat_features = df_stats_tests[df_stats_tests['category'] == category].sort_values('cohens_d', ascending=True)\n",
        "\n",
        "    if len(cat_features) == 0:\n",
        "        ax.text(0.5, 0.5, f'No {category} features',\n",
        "                ha='center', va='center', fontsize=12)\n",
        "        ax.set_title(f'{category}\\n(0 features)', fontsize=11, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "        continue\n",
        "\n",
        "    y_pos = np.arange(len(cat_features))\n",
        "    colors_cat = ['red' if sig else 'gray' for sig in cat_features['bonferroni_significant']]\n",
        "\n",
        "    ax.barh(y_pos, cat_features['cohens_d'], color=colors_cat, alpha=0.7)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels([f.replace('temp_', '').replace('tg_', '')\n",
        "                         for f in cat_features['feature']], fontsize=8)\n",
        "    ax.set_xlabel(\"Cohen's d\", fontsize=9, fontweight='bold')\n",
        "    ax.set_title(f'{category}\\n({len(cat_features)} features)',\n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.suptitle(\"Temporal Features: Effect Sizes by Category\",\n",
        "             fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "category_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'cohens_d_by_category.png')\n",
        "plt.savefig(category_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {category_plot_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnvDgcputgyM",
        "outputId": "90d61398-064d-4ff6-d5c4-2fbddc189e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/cohens_d_by_category.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TRAIN/TEST SPLIT\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df_clean, test_size=0.2, random_state=42, stratify=df_clean['is_ai']\n",
        ")\n",
        "\n",
        "print(f\"Class distribution (train):\")\n",
        "print(f\"AI: ({100*(train_df['is_ai']==1).sum()/len(train_df):.1f}%)\")\n",
        "print(f\"Human: ({100*(train_df['is_ai']==0).sum()/len(train_df):.1f}%)\")\n",
        "print(f\"Class distribution (test):\")\n",
        "print(f\"AI: ({100*(test_df['is_ai']==1).sum()/len(test_df):.1f}%)\")\n",
        "print(f\"Human: ({100*(test_df['is_ai']==0).sum()/len(test_df):.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld9FiyXqti6t",
        "outputId": "8a6b2c8d-09c9-459c-8cc0-29f83dc720e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution (train):\n",
            "AI: (50.0%)\n",
            "Human: (50.0%)\n",
            "Class distribution (test):\n",
            "AI: (50.0%)\n",
            "Human: (50.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MONO-TEMPORAL ANALYSIS (TEMPORAL FEATURES ONLY)\n",
        "\n",
        "print(\"MONO-TEMPORAL ANALYSIS (TEMPORAL FEATURES ONLY)\")\n",
        "\n",
        "# Prepare data (temporal features only)\n",
        "X_temporal_train = train_df[temporal_features].values\n",
        "X_temporal_test = test_df[temporal_features].values\n",
        "y_temporal_train = train_df['is_ai'].values\n",
        "y_temporal_test = test_df['is_ai'].values\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_temporal = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=9,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Cross-validation on training set\n",
        "cv_scores_temporal = cross_val_score(\n",
        "    xgb_temporal, X_temporal_train, y_temporal_train,\n",
        "    cv=5, scoring='f1'\n",
        ")\n",
        "print(f\"F1 scores: {cv_scores_temporal}\")\n",
        "print(f\"Mean F1: {cv_scores_temporal.mean():.4f}\")\n",
        "\n",
        "# Train on full training set\n",
        "print(\"Training final model...\")\n",
        "xgb_temporal.fit(X_temporal_train, y_temporal_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_temporal = xgb_temporal.predict(X_temporal_test)\n",
        "f1_temporal = f1_score(y_temporal_test, y_pred_temporal)\n",
        "\n",
        "print(f\"\\n Mono-Temporal Model Performance:\")\n",
        "print(f\"Test F1: {f1_temporal:.4f}\")\n",
        "print(f\"CV F1: {cv_scores_temporal.mean():.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"Classification Report (Test Set):\")\n",
        "print(classification_report(y_temporal_test, y_pred_temporal,\n",
        "                           target_names=['Human', 'AI']))\n",
        "\n",
        "# Save model\n",
        "temporal_model_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'xgb_temporal_only.pkl')\n",
        "with open(temporal_model_path, 'wb') as f:\n",
        "    pickle.dump(xgb_temporal, f)\n",
        "print(f\"Model saved: {temporal_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTR8xRWatmvz",
        "outputId": "4de50bbd-97b4-4400-dd9d-6a4e88e77d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONO-TEMPORAL ANALYSIS (TEMPORAL FEATURES ONLY)\n",
            "F1 scores: [0.68285124 0.68779221 0.68766404 0.69179827 0.68621399]\n",
            "Mean F1: 0.6873\n",
            "Training final model...\n",
            "\n",
            " Mono-Temporal Model Performance:\n",
            "Test F1: 0.6980\n",
            "CV F1: 0.6873\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.70      0.70      0.70      1200\n",
            "          AI       0.70      0.70      0.70      1200\n",
            "\n",
            "    accuracy                           0.70      2400\n",
            "   macro avg       0.70      0.70      0.70      2400\n",
            "weighted avg       0.70      0.70      0.70      2400\n",
            "\n",
            "Model saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/checkpoints/xgb_temporal_only.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TEMPORAL FEATURE IMPORTANCE (MONO-TEMPORAL MODEL)\n",
        "\n",
        "# Extract feature importance\n",
        "temporal_importance = pd.DataFrame({\n",
        "    'feature': temporal_features,\n",
        "    'importance': xgb_temporal.feature_importances_,\n",
        "    'category': [feature_groups.get(f, 'Unknown') for f in temporal_features]\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Normalize to percentages\n",
        "temporal_importance['importance_pct'] = 100 * temporal_importance['importance'] / temporal_importance['importance'].sum()\n",
        "\n",
        "# Save\n",
        "temporal_importance_path = os.path.join(OUTPUT_DIR, 'tables', 'temporal_feature_importance.csv')\n",
        "temporal_importance.to_csv(temporal_importance_path, index=False)\n",
        "print(f\" Feature importance saved: {temporal_importance_path}\")\n",
        "\n",
        "print(\"Top 20 temporal features by importance:\")\n",
        "print(temporal_importance.head(20)[['feature', 'category', 'importance_pct']].to_string(index=False))\n",
        "\n",
        "# Category-level importance\n",
        "print(\"Feature importance by category:\")\n",
        "category_importance = temporal_importance.groupby('category').agg({\n",
        "    'importance': 'sum',\n",
        "    'importance_pct': 'sum',\n",
        "    'feature': 'count'\n",
        "}).round(2)\n",
        "category_importance.columns = ['Total Importance', 'Importance %', 'N Features']\n",
        "category_importance = category_importance.sort_values('Importance %', ascending=False)\n",
        "print(category_importance.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XykJAtyutomL",
        "outputId": "6af01038-f290-46ec-a6bf-4cb342c596e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature importance saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/temporal_feature_importance.csv\n",
            "Top 20 temporal features by importance:\n",
            "                             feature           category  importance_pct\n",
            "                 temp_scope_variance        Constraints        4.961464\n",
            "                 tg_ordering_entropy    Graph-Theoretic        4.739097\n",
            "                     temp_num_events    Event Structure        3.994022\n",
            "                temp_ref_time_shifts       Form-Meaning        3.873578\n",
            "         temp_rel_parallel_edge_rate          Relations        3.823071\n",
            "             temp_deixis_consistency       Form-Meaning        3.755541\n",
            "      temp_constraint_violation_rate        Constraints        3.586406\n",
            "                          tg_density Graph Organization        3.526302\n",
            "                   tg_edge_retention    Graph-Theoretic        3.459198\n",
            "               temp_rel_type_entropy          Relations        3.330871\n",
            "                 tg_global_coherence    Graph-Theoretic        3.261756\n",
            "                   tg_centralization Graph Organization        3.152283\n",
            "                     tg_longest_path    Graph-Theoretic        2.995962\n",
            "        temp_rel_confidence_variance          Relations        2.994014\n",
            "temp_rel_transitivity_violation_rate          Relations        2.993120\n",
            "           tg_clustering_coefficient Graph Organization        2.989292\n",
            "           temp_rel_cycle_edge_ratio          Relations        2.986505\n",
            "            temp_rel_raw_cycle_count          Relations        2.963804\n",
            "                   tg_avg_out_degree    Graph-Theoretic        2.952231\n",
            "                 tg_branching_factor    Graph-Theoretic        2.938905\n",
            "Feature importance by category:\n",
            "                    Total Importance  Importance %  N Features\n",
            "category                                                      \n",
            "Graph-Theoretic                 0.28     28.270000           9\n",
            "Relations                       0.25     24.559999           9\n",
            "Event Structure                 0.16     16.209999           6\n",
            "Constraints                     0.11     11.450000           3\n",
            "Form-Meaning                    0.10      9.860000           3\n",
            "Graph Organization              0.10      9.670000           3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal Feature Importance\n",
        "\n",
        "\n",
        "print(\"Generating temporal feature importance visualization...\")\n",
        "\n",
        "# Top 30 features\n",
        "top_n = min(30, len(temporal_importance))\n",
        "df_plot_importance = temporal_importance.head(top_n).sort_values('importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, max(10, top_n * 0.35)))\n",
        "\n",
        "# Color by category\n",
        "colors_importance = [category_colors.get(cat, 'gray') for cat in df_plot_importance['category']]\n",
        "\n",
        "y_pos = np.arange(len(df_plot_importance))\n",
        "ax.barh(y_pos, df_plot_importance['importance_pct'], color=colors_importance, alpha=0.8)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot_importance['feature'], fontsize=9)\n",
        "ax.set_xlabel('XGBoost Importance (% Gain)', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Top {top_n} Temporal Features by Importance\\n' +\n",
        "             f'Mono-Temporal Model (Test F1={f1_temporal:.4f})',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add category legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=color, label=cat, alpha=0.8)\n",
        "                   for cat, color in category_colors.items()\n",
        "                   if cat in df_plot_importance['category'].values]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "temporal_importance_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'temporal_importance_top30.png')\n",
        "plt.savefig(temporal_importance_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {temporal_importance_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XZZZtyPtqfX",
        "outputId": "36de60a2-240e-455e-f4e8-5e2b27b1bb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating temporal feature importance visualization...\n",
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/temporal_importance_top30.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Category Importance Breakdown\n",
        "\n",
        "\n",
        "print(\"Generating category importance breakdown...\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Total importance by category\n",
        "sorted_categories = category_importance.sort_values('Importance %', ascending=False)\n",
        "colors_cat = [category_colors.get(cat, 'gray') for cat in sorted_categories.index]\n",
        "\n",
        "ax1.bar(range(len(sorted_categories)), sorted_categories['Importance %'],\n",
        "        color=colors_cat, alpha=0.8)\n",
        "ax1.set_xticks(range(len(sorted_categories)))\n",
        "ax1.set_xticklabels(sorted_categories.index, rotation=45, ha='right', fontsize=10)\n",
        "ax1.set_ylabel('Total Importance (%)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Temporal Feature Importance by Category\\n(Total Contribution)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for i, (cat, val) in enumerate(zip(sorted_categories.index, sorted_categories['Importance %'])):\n",
        "    ax1.text(i, val + 0.5, f'{val:.1f}%', ha='center', va='bottom',\n",
        "             fontsize=9, fontweight='bold')\n",
        "\n",
        "# Plot 2: Per-feature average importance\n",
        "sorted_categories['Avg per Feature'] = sorted_categories['Total Importance'] / sorted_categories['N Features']\n",
        "sorted_avg = sorted_categories.sort_values('Avg per Feature', ascending=False)\n",
        "colors_avg = [category_colors.get(cat, 'gray') for cat in sorted_avg.index]\n",
        "\n",
        "ax2.bar(range(len(sorted_avg)), sorted_avg['Avg per Feature'],\n",
        "        color=colors_avg, alpha=0.8)\n",
        "ax2.set_xticks(range(len(sorted_avg)))\n",
        "ax2.set_xticklabels(sorted_avg.index, rotation=45, ha='right', fontsize=10)\n",
        "ax2.set_ylabel('Average Importance per Feature', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Temporal Feature Importance by Category\\n(Per-Feature Average)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for i, (cat, val) in enumerate(zip(sorted_avg.index, sorted_avg['Avg per Feature'])):\n",
        "    ax2.text(i, val + 0.0005, f'{val:.4f}', ha='center', va='bottom',\n",
        "             fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "category_importance_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'temporal_category_importance.png')\n",
        "plt.savefig(category_importance_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {category_importance_plot_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdpBnCwdttt-",
        "outputId": "e9109eda-9cb8-4382-9e17-88bf4da7b800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating category importance breakdown...\n",
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/temporal_category_importance.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_backbone_train = train_df[backbone_features].values\n",
        "X_backbone_test = test_df[backbone_features].values\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_backbone = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=9,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Cross-validation\n",
        "print(\"Running 5-fold cross-validation\")\n",
        "cv_scores_backbone = cross_val_score(\n",
        "    xgb_backbone, X_backbone_train, y_temporal_train,\n",
        "    cv=5, scoring='f1'\n",
        ")\n",
        "print(f\"F1 scores: {cv_scores_backbone}\")\n",
        "print(f\"Mean F1: {cv_scores_backbone.mean():.4f} (±{cv_scores_backbone.std():.4f})\")\n",
        "\n",
        "\n",
        "xgb_backbone.fit(X_backbone_train, y_temporal_train)\n",
        "\n",
        "y_pred_backbone = xgb_backbone.predict(X_backbone_test)\n",
        "f1_backbone = f1_score(y_temporal_test, y_pred_backbone)\n",
        "\n",
        "print(f\"Backbone Model Performance:\")\n",
        "print(f\"CV F1: {f1_backbone:.4f} (±{cv_scores_backbone.std():.4f})\")\n",
        "\n",
        "all_features = backbone_features + temporal_features\n",
        "X_combined_train = train_df[all_features].values\n",
        "X_combined_test = test_df[all_features].values\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_combined = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=9,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores_combined = cross_val_score(\n",
        "    xgb_combined, X_combined_train, y_temporal_train,\n",
        "    cv=5, scoring='f1'\n",
        ")\n",
        "print(f\"F1 scores: {cv_scores_combined}\")\n",
        "print(f\"Mean F1: {cv_scores_combined.mean():.4f} (±{cv_scores_combined.std():.4f})\")\n",
        "\n",
        "# Train on full training set\n",
        "print(\"Training final model\")\n",
        "xgb_combined.fit(X_combined_train, y_temporal_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_combined = xgb_combined.predict(X_combined_test)\n",
        "f1_combined = f1_score(y_temporal_test, y_pred_combined)\n",
        "\n",
        "print(f\"Combined Model Performance:\")\n",
        "\n",
        "print(f\"CV F1: {f1_combined:.4f} (±{cv_scores_combined.std():.4f})\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(f\"Classification Report (Test Set):\")\n",
        "print(classification_report(y_temporal_test, y_pred_combined,\n",
        "                           target_names=['Human', 'AI']))\n",
        "\n",
        "# Save model\n",
        "combined_model_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'xgb_combined.pkl')\n",
        "with open(combined_model_path, 'wb') as f:\n",
        "    pickle.dump(xgb_combined, f)\n",
        "print(f\"\\n Model saved: {combined_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T603fSqtwIr",
        "outputId": "094307b9-fc57-4e7b-c21d-e851f8cc6aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 5-fold cross-validation\n",
            "F1 scores: [0.81203008 0.81384373 0.85212766 0.83169342 0.82539683]\n",
            "Mean F1: 0.8270 (±0.0145)\n",
            "Backbone Model Performance:\n",
            "CV F1: 0.8405 (±0.0145)\n",
            "F1 scores: [0.82168022 0.82365477 0.86155485 0.82747771 0.82939633]\n",
            "Mean F1: 0.8328 (±0.0147)\n",
            "Training final model\n",
            "Combined Model Performance:\n",
            "CV F1: 0.8488 (±0.0147)\n",
            "Classification Report (Test Set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.83      0.88      0.86      1200\n",
            "          AI       0.87      0.83      0.85      1200\n",
            "\n",
            "    accuracy                           0.85      2400\n",
            "   macro avg       0.85      0.85      0.85      2400\n",
            "weighted avg       0.85      0.85      0.85      2400\n",
            "\n",
            "\n",
            " Model saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/checkpoints/xgb_combined.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature importance\n",
        "combined_importance = pd.DataFrame({\n",
        "    'feature': all_features,\n",
        "    'importance': xgb_combined.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Add feature type\n",
        "combined_importance['type'] = combined_importance['feature'].apply(\n",
        "    lambda f: 'Backbone' if f in backbone_features else 'Temporal'\n",
        ")\n",
        "\n",
        "# Add category for temporal features\n",
        "combined_importance['category'] = combined_importance['feature'].apply(\n",
        "    lambda f: 'Backbone' if f in backbone_features else feature_groups.get(f, 'Unknown')\n",
        ")\n",
        "\n",
        "# Normalize to percentages\n",
        "combined_importance['importance_pct'] = 100 * combined_importance['importance'] / combined_importance['importance'].sum()\n",
        "\n",
        "# Save\n",
        "combined_importance_path = os.path.join(OUTPUT_DIR, 'tables', 'combined_feature_importance.csv')\n",
        "combined_importance.to_csv(combined_importance_path, index=False)\n",
        "print(f\" Feature importance saved: {combined_importance_path}\")\n",
        "\n",
        "print(\"Top 20 features by importance (combined model):\")\n",
        "print(combined_importance.head(20)[['feature', 'type', 'category', 'importance_pct']].to_string(index=False))\n",
        "\n",
        "# Type-level summary\n",
        "print(\"Importance by feature type:\")\n",
        "type_summary = combined_importance.groupby('type').agg({\n",
        "    'importance': 'sum',\n",
        "    'importance_pct': 'sum',\n",
        "}).round(2)\n",
        "type_summary.columns = ['Total Importance', 'Importance %']\n",
        "print(type_summary.to_string())\n",
        "\n",
        "# Category-level summary (including backbone)\n",
        "print(\"Importance by category:\")\n",
        "category_summary_combined = combined_importance.groupby('category').agg({\n",
        "    'importance': 'sum',\n",
        "    'importance_pct': 'sum',\n",
        "}).round(2)\n",
        "category_summary_combined.columns = ['Total Importance', 'Importance %']\n",
        "category_summary_combined = category_summary_combined.sort_values('Importance %', ascending=False)\n",
        "print(category_summary_combined.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAxl6AyLt3Ie",
        "outputId": "a0f49dfc-838e-46c0-db3d-d8cdd850e224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature importance saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/combined_feature_importance.csv\n",
            "Top 20 features by importance (combined model):\n",
            "                 feature     type           category  importance_pct\n",
            "                 yules_k Backbone           Backbone        7.541703\n",
            "       trigram_diversity Backbone           Backbone        4.310170\n",
            "     tg_ordering_entropy Temporal    Graph-Theoretic        3.977894\n",
            "     temp_scope_variance Temporal        Constraints        3.918911\n",
            "     sentence_length_std Backbone           Backbone        3.859744\n",
            "              tg_density Temporal Graph Organization        3.632725\n",
            "        type_token_ratio Backbone           Backbone        3.628889\n",
            "         temp_num_events Temporal    Event Structure        3.516800\n",
            "       tg_avg_out_degree Temporal    Graph-Theoretic        3.265255\n",
            "             comma_ratio Backbone           Backbone        2.533337\n",
            "     tg_global_coherence Temporal    Graph-Theoretic        2.505810\n",
            "temp_events_per_sentence Temporal    Event Structure        2.505033\n",
            "          avg_tree_depth Backbone           Backbone        2.500353\n",
            "  content_function_ratio Backbone           Backbone        2.470646\n",
            "          temp_num_timex Temporal    Event Structure        2.401806\n",
            "    char_trigram_entropy Backbone           Backbone        2.365898\n",
            "       tg_edge_retention Temporal    Graph-Theoretic        2.236959\n",
            "    temp_ref_time_shifts Temporal       Form-Meaning        2.235425\n",
            " temp_deixis_consistency Temporal       Form-Meaning        2.180832\n",
            "       verbs_per_100_tok Backbone           Backbone        2.178596\n",
            "Importance by feature type:\n",
            "          Total Importance  Importance %\n",
            "type                                    \n",
            "Backbone              0.31     31.389999\n",
            "Temporal              0.69     68.610001\n",
            "Importance by category:\n",
            "                    Total Importance  Importance %\n",
            "category                                          \n",
            "Backbone                        0.31     31.389999\n",
            "Graph-Theoretic                 0.21     20.670000\n",
            "Relations                       0.15     15.000000\n",
            "Event Structure                 0.13     12.940000\n",
            "Constraints                     0.07      7.490000\n",
            "Graph Organization              0.07      6.650000\n",
            "Form-Meaning                    0.06      5.870000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Combined Model Feature Importance (Color-Coded)\n",
        "\n",
        "\n",
        "print(\"Generating combined model feature importance visualization...\")\n",
        "\n",
        "# Top 20 features\n",
        "top_n_combined = min(20, len(combined_importance))\n",
        "df_plot_combined = combined_importance.head(top_n_combined).sort_values('importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, max(12, top_n_combined * 0.3)))\n",
        "\n",
        "# Color by category (backbone gets its own color)\n",
        "color_mapping = category_colors.copy()\n",
        "color_mapping['Backbone'] = '#17becf'  # Cyan for backbone\n",
        "\n",
        "colors_combined = [color_mapping.get(cat, 'gray') for cat in df_plot_combined['category']]\n",
        "\n",
        "y_pos = np.arange(len(df_plot_combined))\n",
        "ax.barh(y_pos, df_plot_combined['importance_pct'], color=colors_combined, alpha=0.8)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot_combined['feature'], fontsize=8)\n",
        "ax.set_xlabel('XGBoost Importance (% Gain)', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Top {top_n_combined} Features: Combined Model\\n' +\n",
        "             f'Backbone + Temporal (Test F1={f1_combined:.4f})',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=color_mapping['Backbone'], label='Backbone', alpha=0.8)]\n",
        "legend_elements += [Patch(facecolor=color, label=cat, alpha=0.8)\n",
        "                    for cat, color in category_colors.items()\n",
        "                    if cat in df_plot_combined['category'].values]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=9, ncol=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "combined_importance_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'combined_importance_top40.png')\n",
        "plt.savefig(combined_importance_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {combined_importance_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQvRBxqKt5DQ",
        "outputId": "6eb2d8f6-bae8-4b94-ef0a-937fc1d025ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating combined model feature importance visualization...\n",
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/combined_importance_top40.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ABLATION STUDY - CATEGORY-LEVEL (COMBINED MODEL)\n",
        "\n",
        "print(\"ABLATION STUDY - CATEGORY-LEVEL\")\n",
        "\n",
        "\n",
        "print(\"Testing removal of each temporal feature category...\")\n",
        "\n",
        "ablation_results = []\n",
        "\n",
        "# Baseline: Full combined model\n",
        "ablation_results.append({\n",
        "    'configuration': 'Full Model (Backbone + All Temporal)',\n",
        "    'n_features': len(all_features),\n",
        "    'f1_score': f1_combined,\n",
        "    'delta_f1': 0.0,\n",
        "    'features_removed': 'None'\n",
        "})\n",
        "\n",
        "# Test removing each category\n",
        "for category in ['Event Structure', 'Relations', 'Graph-Theoretic',\n",
        "                 'Constraints', 'Form-Meaning', 'Graph Organization']:\n",
        "\n",
        "    print(f\"\\n  Testing: Backbone + All Temporal EXCEPT {category}\")\n",
        "\n",
        "    # Get features in this category\n",
        "    category_feats = [f for f, c in feature_groups.items() if c == category]\n",
        "\n",
        "    if len(category_feats) == 0:\n",
        "        print(f\"     No features in {category}\")\n",
        "        continue\n",
        "\n",
        "    # Create feature set without this category\n",
        "    features_without_category = backbone_features + [f for f in temporal_features if f not in category_feats]\n",
        "\n",
        "    print(f\"  Features without {category}: {len(features_without_category)}\")\n",
        "    print(f\"  Removed: {len(category_feats)} features\")\n",
        "\n",
        "    # Train model\n",
        "    X_ablation_train = train_df[features_without_category].values\n",
        "    X_ablation_test = test_df[features_without_category].values\n",
        "\n",
        "    xgb_ablation = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=9,\n",
        "        learning_rate=0.15,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    xgb_ablation.fit(X_ablation_train, y_temporal_train)\n",
        "    y_pred_ablation = xgb_ablation.predict(X_ablation_test)\n",
        "    f1_ablation = f1_score(y_temporal_test, y_pred_ablation)\n",
        "\n",
        "    delta = f1_ablation - f1_combined\n",
        "\n",
        "    print(f\"  F1: {f1_ablation:.4f} (Δ{delta:+.4f})\")\n",
        "\n",
        "    ablation_results.append({\n",
        "        'configuration': f'Without {category}',\n",
        "        'n_features': len(features_without_category),\n",
        "        'f1_score': f1_ablation,\n",
        "        'delta_f1': delta,\n",
        "        'features_removed': category\n",
        "    })\n",
        "\n",
        "# Also test: Backbone only (already computed)\n",
        "ablation_results.append({\n",
        "    'configuration': 'Backbone Only (All Temporal Removed)',\n",
        "    'n_features': len(backbone_features),\n",
        "    'f1_score': f1_backbone,\n",
        "    'delta_f1': f1_backbone - f1_combined,\n",
        "    'features_removed': 'All Temporal'\n",
        "})\n",
        "\n",
        "df_ablation = pd.DataFrame(ablation_results)\n",
        "\n",
        "# Save ablation results\n",
        "ablation_path = os.path.join(OUTPUT_DIR, 'tables', 'ablation_category_level.csv')\n",
        "df_ablation.to_csv(ablation_path, index=False)\n",
        "print(f\"\\n Ablation results saved: {ablation_path}\")\n",
        "\n",
        "print(\"Category-Level Ablation Summary:\")\n",
        "print(df_ablation.sort_values('delta_f1', ascending=True).to_string(index=False))\n",
        "\n",
        "# Identify most important category (largest drop when removed)\n",
        "most_important_category = df_ablation[df_ablation['configuration'].str.startswith('Without')].sort_values('delta_f1').iloc[0]\n",
        "print(f\"Most Critical Category:\")\n",
        "print(f\"{most_important_category['features_removed']}\")\n",
        "print(f\"Removal causes: {most_important_category['delta_f1']:.4f} drop in F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZVMAeABuG5w",
        "outputId": "3eb1a410-533e-4670-e354-36d18d90308a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ABLATION STUDY - CATEGORY-LEVEL\n",
            "Testing removal of each temporal feature category...\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Event Structure\n",
            "  Features without Event Structure: 36\n",
            "  Removed: 6 features\n",
            "  F1: 0.8436 (Δ-0.0052)\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Relations\n",
            "  Features without Relations: 33\n",
            "  Removed: 9 features\n",
            "  F1: 0.8493 (Δ+0.0005)\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Graph-Theoretic\n",
            "  Features without Graph-Theoretic: 33\n",
            "  Removed: 9 features\n",
            "  F1: 0.8462 (Δ-0.0026)\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Constraints\n",
            "  Features without Constraints: 39\n",
            "  Removed: 3 features\n",
            "  F1: 0.8383 (Δ-0.0105)\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Form-Meaning\n",
            "  Features without Form-Meaning: 39\n",
            "  Removed: 3 features\n",
            "  F1: 0.8481 (Δ-0.0007)\n",
            "\n",
            "  Testing: Backbone + All Temporal EXCEPT Graph Organization\n",
            "  Features without Graph Organization: 39\n",
            "  Removed: 3 features\n",
            "  F1: 0.8505 (Δ+0.0017)\n",
            "\n",
            " Ablation results saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/ablation_category_level.csv\n",
            "Category-Level Ablation Summary:\n",
            "                       configuration  n_features  f1_score  delta_f1   features_removed\n",
            "                 Without Constraints          39  0.838298 -0.010524        Constraints\n",
            "Backbone Only (All Temporal Removed)           9  0.840470 -0.008352       All Temporal\n",
            "             Without Event Structure          36  0.843630 -0.005192    Event Structure\n",
            "             Without Graph-Theoretic          33  0.846220 -0.002602    Graph-Theoretic\n",
            "                Without Form-Meaning          39  0.848096 -0.000726       Form-Meaning\n",
            "Full Model (Backbone + All Temporal)          42  0.848822  0.000000               None\n",
            "                   Without Relations          33  0.849292  0.000469          Relations\n",
            "          Without Graph Organization          39  0.850515  0.001693 Graph Organization\n",
            "Most Critical Category:\n",
            "Constraints\n",
            "Removal causes: -0.0105 drop in F1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZATION 7: Category-Level Ablation Study\n",
        "\n",
        "print(\"Generating category-level ablation visualization...\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Plot 1: F1 scores for each configuration\n",
        "df_ablation_sorted = df_ablation.sort_values('f1_score', ascending=True)\n",
        "y_pos = np.arange(len(df_ablation_sorted))\n",
        "\n",
        "# Color code: full model = green, without categories = orange/red based on drop\n",
        "colors_ablation = []\n",
        "for _, row in df_ablation_sorted.iterrows():\n",
        "    if row['configuration'] == 'Full Model (Backbone + All Temporal)':\n",
        "        colors_ablation.append('#2ca02c')  # Green\n",
        "    elif row['configuration'] == 'Backbone Only (All Temporal Removed)':\n",
        "        colors_ablation.append('#d62728')  # Red\n",
        "    else:\n",
        "        # Orange shades based on drop magnitude\n",
        "        drop = abs(row['delta_f1'])\n",
        "        if drop < 0.002:\n",
        "            colors_ablation.append('#ffeda0')  # Light yellow\n",
        "        elif drop < 0.005:\n",
        "            colors_ablation.append('#feb24c')  # Orange\n",
        "        else:\n",
        "            colors_ablation.append('#f03b20')  # Dark orange\n",
        "\n",
        "ax1.barh(y_pos, df_ablation_sorted['f1_score'], color=colors_ablation, alpha=0.85)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(df_ablation_sorted['configuration'], fontsize=10)\n",
        "ax1.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Ablation Study: F1 Score by Configuration\\n(Which Categories Are Essential?)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.axvline(x=f1_combined, color='green', linestyle='--', linewidth=2,\n",
        "            label=f'Full Model ({f1_combined:.4f})')\n",
        "ax1.axvline(x=f1_backbone, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Backbone Only ({f1_backbone:.4f})')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, (yval, score) in enumerate(zip(y_pos, df_ablation_sorted['f1_score'])):\n",
        "    ax1.text(score + 0.001, yval, f'{score:.4f}',\n",
        "             va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Plot 2: Delta F1 (impact of removal)\n",
        "df_ablation_impact = df_ablation[df_ablation['configuration'].str.startswith('Without')].copy()\n",
        "df_ablation_impact = df_ablation_impact.sort_values('delta_f1', ascending=True)\n",
        "\n",
        "y_pos_impact = np.arange(len(df_ablation_impact))\n",
        "colors_impact = ['red' if delta < -0.003 else 'orange' if delta < -0.001 else 'yellow'\n",
        "                 for delta in df_ablation_impact['delta_f1']]\n",
        "\n",
        "ax2.barh(y_pos_impact, df_ablation_impact['delta_f1'], color=colors_impact, alpha=0.85)\n",
        "ax2.set_yticks(y_pos_impact)\n",
        "ax2.set_yticklabels([conf.replace('Without ', '') for conf in df_ablation_impact['configuration']],\n",
        "                     fontsize=10)\n",
        "ax2.set_xlabel('ΔF1 (vs Full Model)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Impact of Removing Each Category\\n(Negative = Performance Drop)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, (yval, delta) in enumerate(zip(y_pos_impact, df_ablation_impact['delta_f1'])):\n",
        "    x_pos = delta - 0.0005 if delta < 0 else delta + 0.0005\n",
        "    ax2.text(x_pos, yval, f'{delta:.4f}',\n",
        "             va='center', ha='right' if delta < 0 else 'left',\n",
        "             fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Ablation Study: Importance of Each Temporal Feature Category',\n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "ablation_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'ablation_category_level.png')\n",
        "plt.savefig(ablation_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {ablation_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkPjeui7uK3a",
        "outputId": "5ece83ef-f6ab-44a6-a60f-2e8847044bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating category-level ablation visualization...\n",
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/ablation_category_level.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ABLATION STUDY - INDIVIDUAL FEATURES (TOP TEMPORAL)\n",
        "\n",
        "\n",
        "\n",
        "print(\"ABLATION STUDY - INDIVIDUAL TOP TEMPORAL FEATURES\")\n",
        "\n",
        "\n",
        "print(\"Testing removal of top 20 temporal features individually...\")\n",
        "\n",
        "# Get top 20 temporal features from combined model\n",
        "top_temporal_features = combined_importance[\n",
        "    combined_importance['type'] == 'Temporal'\n",
        "].head(20)['feature'].tolist()\n",
        "\n",
        "individual_ablation_results = []\n",
        "\n",
        "for feat in tqdm(top_temporal_features, desc=\"Ablating features\"):\n",
        "    # Create feature set without this feature\n",
        "    features_without_feat = [f for f in all_features if f != feat]\n",
        "\n",
        "    # Train model\n",
        "    X_ablation_train = train_df[features_without_feat].values\n",
        "    X_ablation_test = test_df[features_without_feat].values\n",
        "\n",
        "    xgb_ablation = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=9,\n",
        "        learning_rate=0.15,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    xgb_ablation.fit(X_ablation_train, y_temporal_train)\n",
        "    y_pred_ablation = xgb_ablation.predict(X_ablation_test)\n",
        "    f1_ablation = f1_score(y_temporal_test, y_pred_ablation)\n",
        "\n",
        "    delta = f1_ablation - f1_combined\n",
        "\n",
        "    # Get feature importance from combined model\n",
        "    feat_importance = combined_importance[combined_importance['feature'] == feat]['importance_pct'].values[0]\n",
        "    feat_category = feature_groups.get(feat, 'Unknown')\n",
        "\n",
        "    individual_ablation_results.append({\n",
        "        'feature': feat,\n",
        "        'category': feat_category,\n",
        "        'importance_pct': feat_importance,\n",
        "        'f1_without': f1_ablation,\n",
        "        'delta_f1': delta\n",
        "    })\n",
        "\n",
        "df_individual_ablation = pd.DataFrame(individual_ablation_results)\n",
        "df_individual_ablation = df_individual_ablation.sort_values('delta_f1', ascending=True)\n",
        "\n",
        "# Save individual ablation results\n",
        "individual_ablation_path = os.path.join(OUTPUT_DIR, 'tables', 'ablation_individual_features.csv')\n",
        "df_individual_ablation.to_csv(individual_ablation_path, index=False)\n",
        "print(f\"\\n Individual ablation results saved: {individual_ablation_path}\")\n",
        "\n",
        "print(\"Individual Feature Ablation Summary (Top 20 Temporal Features):\")\n",
        "print(df_individual_ablation[['feature', 'category', 'importance_pct', 'delta_f1']].to_string(index=False))\n",
        "\n",
        "# Identify most critical individual features\n",
        "print(\"Most Critical Individual Features (largest drop when removed):\")\n",
        "most_critical = df_individual_ablation.head(5)\n",
        "for idx, row in most_critical.iterrows():\n",
        "    print(f\"{row['feature']} [{row['category']}]: {row['delta_f1']:.4f} drop\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq0vRLseuN0v",
        "outputId": "3caf2a26-85f9-444f-ff5e-e760e856f2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ABLATION STUDY - INDIVIDUAL TOP TEMPORAL FEATURES\n",
            "Testing removal of top 20 temporal features individually...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ablating features: 100%|██████████| 20/20 [03:05<00:00,  9.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Individual ablation results saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/ablation_individual_features.csv\n",
            "Individual Feature Ablation Summary (Top 20 Temporal Features):\n",
            "                     feature           category  importance_pct  delta_f1\n",
            "temp_rel_confidence_variance          Relations        1.801754 -0.006178\n",
            "           tg_avg_out_degree    Graph-Theoretic        3.265255 -0.005139\n",
            "   temp_constraint_csp_score        Constraints        1.915049 -0.005139\n",
            " temp_rel_parallel_edge_rate          Relations        2.082510 -0.005139\n",
            "            tg_avg_in_degree    Graph-Theoretic        1.851448 -0.005139\n",
            "         tg_global_coherence    Graph-Theoretic        2.505810 -0.004739\n",
            "                  tg_density Graph Organization        3.632725 -0.003921\n",
            "             temp_num_events    Event Structure        3.516800 -0.003497\n",
            "         temp_scope_variance        Constraints        3.918911 -0.003259\n",
            "     temp_deixis_consistency       Form-Meaning        2.180832 -0.002800\n",
            "    temp_events_per_sentence    Event Structure        2.505033 -0.002339\n",
            "        temp_ref_time_shifts       Form-Meaning        2.235425 -0.001844\n",
            "           tg_edge_retention    Graph-Theoretic        2.236959 -0.001844\n",
            "         tg_ordering_entropy    Graph-Theoretic        3.977894 -0.001743\n",
            "       temp_rel_type_entropy          Relations        1.948210 -0.001117\n",
            "              temp_num_timex    Event Structure        2.401806 -0.000857\n",
            "    temp_rel_raw_cycle_count          Relations        1.965729 -0.000441\n",
            "    temp_rel_mean_confidence          Relations        1.980331 -0.000155\n",
            "   temp_rel_cycle_edge_ratio          Relations        1.832684  0.000879\n",
            "             tg_longest_path    Graph-Theoretic        2.059478  0.001135\n",
            "Most Critical Individual Features (largest drop when removed):\n",
            "temp_rel_confidence_variance [Relations]: -0.0062 drop\n",
            "tg_avg_out_degree [Graph-Theoretic]: -0.0051 drop\n",
            "temp_constraint_csp_score [Constraints]: -0.0051 drop\n",
            "temp_rel_parallel_edge_rate [Relations]: -0.0051 drop\n",
            "tg_avg_in_degree [Graph-Theoretic]: -0.0051 drop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VISUALIZATION 8: Individual Feature Ablation\n",
        "\n",
        "\n",
        "print(\"Generating individual feature ablation visualization...\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Plot 1: Delta F1 for each feature removal\n",
        "df_plot_ablation = df_individual_ablation.sort_values('delta_f1', ascending=True)\n",
        "y_pos = np.arange(len(df_plot_ablation))\n",
        "\n",
        "# Color by category\n",
        "colors_feat_ablation = [category_colors.get(cat, 'gray') for cat in df_plot_ablation['category']]\n",
        "\n",
        "ax1.barh(y_pos, df_plot_ablation['delta_f1'], color=colors_feat_ablation, alpha=0.8)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(df_plot_ablation['feature'], fontsize=9)\n",
        "ax1.set_xlabel('ΔF1 (vs Full Model)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Impact of Removing Each Top Temporal Feature\\n(Negative = Performance Drop)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels for top 10\n",
        "for i in range(min(10, len(y_pos))):\n",
        "    yval = y_pos[i]\n",
        "    delta = df_plot_ablation.iloc[i]['delta_f1']\n",
        "    x_pos = delta - 0.0002 if delta < 0 else delta + 0.0002\n",
        "    ax1.text(x_pos, yval, f'{delta:.4f}',\n",
        "             va='center', ha='right' if delta < 0 else 'left',\n",
        "             fontsize=8, fontweight='bold')\n",
        "\n",
        "# Plot 2: Importance vs Impact scatter\n",
        "ax2.scatter(df_individual_ablation['importance_pct'],\n",
        "            -df_individual_ablation['delta_f1'],  # Negative so drops are positive\n",
        "            c=[category_colors.get(cat, 'gray') for cat in df_individual_ablation['category']],\n",
        "            s=100, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "ax2.set_xlabel('Feature Importance (% Gain)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Performance Drop When Removed (|ΔF1|)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Importance vs Impact Analysis\\n(Do Important Features Actually Matter?)',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add diagonal reference line\n",
        "max_val = max(df_individual_ablation['importance_pct'].max(),\n",
        "              (-df_individual_ablation['delta_f1']).max())\n",
        "ax2.plot([0, max_val], [0, max_val * 0.01], 'k--', alpha=0.3, linewidth=1)\n",
        "\n",
        "# Annotate top 5 most impactful\n",
        "top_5_impact = df_individual_ablation.nsmallest(5, 'delta_f1')\n",
        "for _, row in top_5_impact.iterrows():\n",
        "    ax2.annotate(row['feature'].replace('temp_', '').replace('tg_', ''),\n",
        "                xy=(row['importance_pct'], -row['delta_f1']),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=8, alpha=0.8,\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=color, label=cat, alpha=0.7)\n",
        "                   for cat, color in category_colors.items()\n",
        "                   if cat in df_individual_ablation['category'].values]\n",
        "ax2.legend(handles=legend_elements, loc='upper left', fontsize=9)\n",
        "\n",
        "plt.suptitle('Individual Feature Ablation: Top 20 Temporal Features',\n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "individual_ablation_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'ablation_individual_features.png')\n",
        "plt.savefig(individual_ablation_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {individual_ablation_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuHw9Qa-uP9C",
        "outputId": "90661bcb-b01f-4a5e-afdc-718387849624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating individual feature ablation visualization...\n",
            "   Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/ablation_individual_features.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"SHAP ANALYSIS (COMBINED MODEL)\")\n",
        "\n",
        "\n",
        "print(\"Computing SHAP values for combined model...\")\n",
        "print(\"   This may take several minutes...\")\n",
        "\n",
        "# Sample data for SHAP (use subset for speed)\n",
        "n_shap_samples = min(500, len(X_combined_test))\n",
        "X_shap = X_combined_test[:n_shap_samples]\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb_combined)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "print(f\" SHAP values computed for {n_shap_samples} samples\")\n",
        "\n",
        "# Save SHAP values\n",
        "shap_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'shap_values.pkl')\n",
        "with open(shap_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'shap_values': shap_values,\n",
        "        'X_shap': X_shap,\n",
        "        'feature_names': all_features\n",
        "    }, f)\n",
        "print(f\" SHAP values saved: {shap_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoMqcyhJuRub",
        "outputId": "1d4f9f2b-8492-4040-f942-a18f5a4f2ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 16: SHAP ANALYSIS (COMBINED MODEL)\n",
            "================================================================================\n",
            "\n",
            "→ Computing SHAP values for combined model...\n",
            "  ⚠ This may take several minutes...\n",
            "✓ SHAP values computed for 500 samples\n",
            "✓ SHAP values saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/checkpoints/shap_values.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VISUALIZATION 9: SHAP Summary Plot\n",
        "\n",
        "\n",
        "print(\"Generating SHAP summary plot...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# SHAP summary plot\n",
        "shap.summary_plot(shap_values, X_shap,\n",
        "                  feature_names=all_features,\n",
        "                  max_display=30,\n",
        "                  show=False)\n",
        "\n",
        "plt.title('SHAP Feature Importance: Combined Model\\n(Feature Impact on Predictions)',\n",
        "          fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "shap_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'shap_summary_combined.png')\n",
        "plt.savefig(shap_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {shap_plot_path}\")\n",
        "\n",
        "\n",
        "# VISUALIZATION 10: SHAP Bar Plot (Mean Absolute SHAP)\n",
        "\n",
        "\n",
        "print(\"Generating SHAP bar plot...\")\n",
        "\n",
        "# Calculate mean absolute SHAP values\n",
        "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "# Create dataframe\n",
        "shap_importance = pd.DataFrame({\n",
        "    'feature': all_features,\n",
        "    'mean_abs_shap': mean_abs_shap,\n",
        "    'type': ['Backbone' if f in backbone_features else 'Temporal' for f in all_features],\n",
        "    'category': ['Backbone' if f in backbone_features else feature_groups.get(f, 'Unknown')\n",
        "                 for f in all_features]\n",
        "}).sort_values('mean_abs_shap', ascending=False)\n",
        "\n",
        "# Save SHAP importance\n",
        "shap_importance_path = os.path.join(OUTPUT_DIR, 'tables', 'shap_importance.csv')\n",
        "shap_importance.to_csv(shap_importance_path, index=False)\n",
        "print(f\" SHAP importance saved: {shap_importance_path}\")\n",
        "\n",
        "# Plot top 30\n",
        "top_n_shap = min(30, len(shap_importance))\n",
        "df_plot_shap = shap_importance.head(top_n_shap).sort_values('mean_abs_shap', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, max(10, top_n_shap * 0.35)))\n",
        "\n",
        "# Color by category\n",
        "colors_shap = [color_mapping.get(cat, 'gray') for cat in df_plot_shap['category']]\n",
        "\n",
        "y_pos = np.arange(len(df_plot_shap))\n",
        "ax.barh(y_pos, df_plot_shap['mean_abs_shap'], color=colors_shap, alpha=0.8)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot_shap['feature'], fontsize=9)\n",
        "ax.set_xlabel('Mean |SHAP Value|', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Top {top_n_shap} Features by SHAP Importance\\n(Average Impact on Model Output)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add legend\n",
        "legend_elements = [Patch(facecolor=color_mapping['Backbone'], label='Backbone', alpha=0.8)]\n",
        "legend_elements += [Patch(facecolor=color, label=cat, alpha=0.8)\n",
        "                    for cat, color in category_colors.items()\n",
        "                    if cat in df_plot_shap['category'].values]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=9, ncol=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "shap_bar_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'shap_bar_top30.png')\n",
        "plt.savefig(shap_bar_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"   Saved: {shap_bar_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwvtNVRduVoj",
        "outputId": "38dd678e-67fc-4070-a301-dfb6ee335aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Generating SHAP summary plot...\n",
            "  ✓ Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/shap_summary_combined.png\n",
            "\n",
            "→ Generating SHAP bar plot...\n",
            "✓ SHAP importance saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/tables/shap_importance.csv\n",
            "  ✓ Saved: /content/drive/MyDrive/Tesi Magistrale/temporal_analysis/plots/shap_bar_top30.png\n"
          ]
        }
      ]
    }
  ]
}