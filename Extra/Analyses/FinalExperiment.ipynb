{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# XGBoost evaluation across all feature categories\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix,\n",
        "                             classification_report)\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "BASE_DRIVE = '/content/drive/MyDrive/Tesi Magistrale'\n",
        "MASTER_DIR = f'{BASE_DRIVE}/master_features'\n",
        "OUTPUT_DIR = f'{BASE_DRIVE}/analysis_results'\n",
        "\n",
        "# Create output directories\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(f'{OUTPUT_DIR}/models', exist_ok=True)\n",
        "os.makedirs(f'{OUTPUT_DIR}/figures', exist_ok=True)\n",
        "os.makedirs(f'{OUTPUT_DIR}/reports', exist_ok=True)\n",
        "\n",
        "# Load master dataframe\n",
        "df_master = pd.read_csv(f'{MASTER_DIR}/master_features_complete.csv')\n",
        "print(f\"Loaded master dataframe\")\n",
        "\n",
        "# Load feature categories\n",
        "df_manifest = pd.read_csv(f'{MASTER_DIR}/feature_manifest.csv')\n",
        "print(f\"Feature manifest loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu7B9-9rC1n9",
        "outputId": "e41329a1-dea4-4760-a9f4-0319bc77142f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded master dataframe\n",
            "Feature manifest loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining feature categories\n",
        "\n",
        "# Reconstruct feature categories from manifest\n",
        "FEATURE_CATEGORIES = {}\n",
        "for category in df_manifest['category'].unique():\n",
        "    features = df_manifest[df_manifest['category'] == category]['feature'].tolist()\n",
        "    # Verify features exist in master dataframe\n",
        "    features = [f for f in features if f in df_master.columns]\n",
        "    FEATURE_CATEGORIES[category] = features\n",
        "\n",
        "#Experimental configuration\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_master.drop(columns=['id', 'is_ai'])\n",
        "y = df_master['is_ai']\n",
        "\n",
        "# Define experimental configurations\n",
        "EXPERIMENTS = {\n",
        "    # Combined experiments\n",
        "    'all_features': {\n",
        "        'name': 'ALL FEATURES',\n",
        "        'features': [f for cat_features in FEATURE_CATEGORIES.values()\n",
        "                     for f in cat_features],\n",
        "        'description': 'All cognitive + backbone features'\n",
        "    },\n",
        "    'monocognitive': {\n",
        "        'name': 'COGNITION',\n",
        "        'features': [f for cat, cat_features in FEATURE_CATEGORIES.items()\n",
        "                     if cat != 'BACKBONE' for f in cat_features],\n",
        "        'description': 'Pure cognitive features (no stylometrics)'\n",
        "    },\n",
        "}\n",
        "\n",
        "# VALIDATE ALL CONFIGURATIONS - Add valid_features to each config\n",
        "print(f\"Validating experimental configurations\\n\")\n",
        "for exp_name, exp_config in EXPERIMENTS.items():\n",
        "    features = exp_config['features']\n",
        "\n",
        "    # Remove duplicates and verify existence\n",
        "    features = list(dict.fromkeys(features))  # Remove duplicates while preserving order\n",
        "    valid_features = [f for f in features if f in X.columns]\n",
        "    missing_features = [f for f in features if f not in X.columns]\n",
        "\n",
        "    # Store validated features in config\n",
        "    exp_config['valid_features'] = valid_features\n",
        "    exp_config['n_features'] = len(valid_features)\n",
        "    exp_config['missing_features'] = missing_features\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"{exp_name}: {len(missing_features)} missing features\")\n",
        "\n",
        "print(f\"experimental configurations validated\")\n",
        "for exp_name, exp_config in EXPERIMENTS.items():\n",
        "    status = \"✓\" if exp_config['n_features'] > 0 else \"✗\"\n",
        "\n",
        "# Check if any experiments have no features\n",
        "empty_experiments = [name for name, config in EXPERIMENTS.items() if config['n_features'] == 0]\n",
        "if empty_experiments:\n",
        "    print(f\"{len(empty_experiments)} experiments have no valid features:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQAHDIxhDGnM",
        "outputId": "12158c49-0ba0-4ac9-dac8-e1a5d1dddb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating experimental configurations\n",
            "\n",
            "experimental configurations validated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost hyperparameters\n",
        "XGBOOST_PARAMS = {\n",
        "    'n_estimators': 500,\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.15,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n",
        "\n",
        "\n",
        "# Cross-validation configuration\n",
        "CV_FOLDS = 5\n",
        "\n",
        "def evaluate_model(X_exp, y_exp, experiment_name, n_features, verbose=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate XGBoost model with cross-validation.\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with all metrics and model\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize model\n",
        "    model = XGBClassifier(**XGBOOST_PARAMS)\n",
        "\n",
        "    # Define cross-validation\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True)\n",
        "\n",
        "    # Define scoring metrics\n",
        "    scoring = {\n",
        "        'accuracy': 'accuracy',\n",
        "        'precision': 'precision',\n",
        "        'recall': 'recall',\n",
        "        'f1': 'f1',\n",
        "        'roc_auc': 'roc_auc'\n",
        "    }\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_results = cross_validate(\n",
        "        model, X_exp, y_exp,\n",
        "        cv=cv,\n",
        "        scoring=scoring,\n",
        "        return_train_score=True,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Train final model on full data\n",
        "    model.fit(X_exp, y_exp)\n",
        "    y_pred = model.predict(X_exp)\n",
        "    y_pred_proba = model.predict_proba(X_exp)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'experiment': experiment_name,\n",
        "        'n_features': n_features,\n",
        "\n",
        "        # Cross-validation metrics (test scores)\n",
        "        'cv_accuracy_mean': cv_results['test_accuracy'].mean(),\n",
        "        'cv_accuracy_std': cv_results['test_accuracy'].std(),\n",
        "        'cv_precision_mean': cv_results['test_precision'].mean(),\n",
        "        'cv_precision_std': cv_results['test_precision'].std(),\n",
        "        'cv_recall_mean': cv_results['test_recall'].mean(),\n",
        "        'cv_recall_std': cv_results['test_recall'].std(),\n",
        "        'cv_f1_mean': cv_results['test_f1'].mean(),\n",
        "        'cv_f1_std': cv_results['test_f1'].std(),\n",
        "        'cv_roc_auc_mean': cv_results['test_roc_auc'].mean(),\n",
        "        'cv_roc_auc_std': cv_results['test_roc_auc'].std(),\n",
        "\n",
        "        # Full data metrics (for reference)\n",
        "        'full_accuracy': accuracy_score(y_exp, y_pred),\n",
        "        'full_precision': precision_score(y_exp, y_pred),\n",
        "        'full_recall': recall_score(y_exp, y_pred),\n",
        "        'full_f1': f1_score(y_exp, y_pred),\n",
        "        'full_roc_auc': roc_auc_score(y_exp, y_pred_proba),\n",
        "\n",
        "        # Model and predictions\n",
        "        'model': model,\n",
        "        'cv_results': cv_results,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"CV F1: {results['cv_f1_mean']:.4f} ± {results['cv_f1_std']:.4f}\")\n",
        "        print(f\"CV Accuracy: {results['cv_accuracy_mean']:.4f} ± {results['cv_accuracy_std']:.4f}\")\n",
        "        print(f\" CV ROC-AUC: {results['cv_roc_auc_mean']:.4f} ± {results['cv_roc_auc_std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"Evaluation function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2dtfymaDPKF",
        "outputId": "9683f952-baff-4079-930c-8666ceabf2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running Experiments\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for exp_name, exp_config in EXPERIMENTS.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Experiment: {exp_config['name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Description: {exp_config['description']}\")\n",
        "\n",
        "    # Get features\n",
        "    features = exp_config['valid_features']\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(\"No valid features\")\n",
        "        continue\n",
        "\n",
        "    # Prepare data\n",
        "    X_exp = X[features].copy()\n",
        "\n",
        "    # Check for NaN\n",
        "    if X_exp.isna().any().any():\n",
        "        n_nan = X_exp.isna().sum().sum()\n",
        "        print(f\"NaN values found - filling with median\")\n",
        "        X_exp = X_exp.fillna(X_exp.median())\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluate_model(\n",
        "        X_exp, y,\n",
        "        exp_config['name'],\n",
        "        len(features),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    all_results[exp_name] = results\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"EXPERIMENTS COMPLETE\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxmS1d6yDSBi",
        "outputId": "eb0c4cf6-fce5-4736-ed6c-4d6ac2802641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Experiments\n",
            "\n",
            "================================================================================\n",
            "Experiment: ALL FEATURES\n",
            "================================================================================\n",
            "Description: All cognitive + backbone features\n",
            "CV F1: 0.9234 ± 0.0056\n",
            "CV Accuracy: 0.9246 ± 0.0055\n",
            "CV ROC-AUC: 0.9778 ± 0.0026\n",
            "\n",
            "================================================================================\n",
            "Experiment: COGNITION\n",
            "================================================================================\n",
            "Description: Pure cognitive features (no stylometrics)\n",
            "CV F1: 0.8660 ± 0.0041\n",
            "CV Accuracy: 0.8691 ± 0.0041\n",
            "CV ROC-AUC: 0.9442 ± 0.0058\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENTS COMPLETE\n"
          ]
        }
      ]
    }
  ]
}