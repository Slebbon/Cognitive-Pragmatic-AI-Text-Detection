{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn scipy -q\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully\")\n",
        "\n",
        "    # Verify mount\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"Drive path verified\")\n",
        "    else:\n",
        "        raise Exception(\"Drive mounted but MyDrive not found\")\n",
        "except Exception as e:\n",
        "    print(f\" ERROR: Could not mount Google Drive: {e}\")\n",
        "    raise\n",
        "\n",
        "# Check available space\n",
        "import shutil\n",
        "try:\n",
        "    available_space = shutil.disk_usage('/content/drive/MyDrive').free / 1e9\n",
        "    print(f\"Available space on Drive: {available_space:.1f} GB\")\n",
        "    if available_space < 1.0:\n",
        "        print(f\"WARNING: Low disk space!\")\n",
        "except:\n",
        "    print(f\"Could not check disk space\")\n",
        "\n",
        "# Optional: Check GPU (not needed for this analysis, but useful info)\n",
        "import torch\n",
        "print(f\"GPU Check (optional, not required for this analysis):\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(f\"Running on CPU (sufficient for this analysis)\")\n",
        "\n",
        "\n",
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Samhpnxz7KUY",
        "outputId": "ae34453b-26e1-457e-d752-531cc412d278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully\n",
            "Drive path verified\n",
            "Available space on Drive: 200.9 GB\n",
            "GPU Check (optional, not required for this analysis):\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: choreographer>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from kaleido) (1.2.1)\n",
            "Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.0.1)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kaleido) (25.0)\n",
            "Requirement already satisfied: pytest-timeout>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.4.0)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\n",
            "Requirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44ppeJZS6M-k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import time\n",
        "\n",
        "def load_feature_list(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    features = (\n",
        "        df[\"feature\"]\n",
        "        .dropna()\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .unique()\n",
        "        .tolist()\n",
        "    )\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD ALL DATASETS\n",
        "\n",
        "ORIGINAL_DF_PATH = '/content/raid_sample_medium_PostPOS_CLEAN (1).csv'\n",
        "ENTITY_FEATURES_PATH = '/content/drive/MyDrive/Tesi Magistrale/entity_cohesion/entity_cohesion_features_final.csv'\n",
        "SEMANTIC_FEATURES_PATH = '/content/drive/MyDrive/Tesi Magistrale/semantic_cohesion/semantic_cohesion_features_final.csv'\n",
        "BASE_FEATURES_PATH = '/content/baseline_features.csv'\n",
        "TOPIC_FEATURES_PATH = '/content/drive/MyDrive/Tesi Magistrale/topic_coherence/topic_coherence_features_final.csv'\n",
        "\n",
        "\n",
        "# Output directory for results\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Tesi Magistrale/cohesion_analysis'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'plots'), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'tables'), exist_ok=True)\n",
        "\n",
        "backbone_features = load_feature_list(BASE_FEATURES_PATH)\n",
        "df_original = pd.read_csv(ORIGINAL_DF_PATH)\n",
        "\n",
        "df_entity = pd.read_csv(ENTITY_FEATURES_PATH)\n",
        "df_semantic = pd.read_csv(SEMANTIC_FEATURES_PATH)\n",
        "df_topic = pd.read_csv(TOPIC_FEATURES_PATH)\n",
        "\n",
        "\n",
        "#Merging all dataset\n",
        "\n",
        "# Select backbone + label from original\n",
        "df_backbone = df_original[['id', 'is_ai'] + backbone_features].copy()\n",
        "df_merged = df_backbone.merge(df_entity, on='id', how='inner')\n",
        "df_merged = df_merged.merge(df_semantic, on='id', how='inner')\n",
        "df_merged = df_merged.merge(df_topic, on='id', how='inner')\n",
        "\n",
        "# Get feature groups\n",
        "entity_features = [c for c in df_entity.columns if c != 'id']\n",
        "semantic_features = [c for c in df_semantic.columns if c != 'id']\n",
        "topic_features = [c for c in df_topic.columns if c != 'id']\n",
        "\n",
        "all_cohesion_features = entity_features + semantic_features + topic_features\n",
        "all_features = backbone_features + all_cohesion_features\n",
        "\n",
        "print(f\"Feature breakdown:\")\n",
        "print(f\"Entity cohesion: {len(entity_features)}\")\n",
        "print(f\"Semantic cohesion: {len(semantic_features)}\")\n",
        "print(f\"Topic coherence: {len(topic_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM08h-JU6VD3",
        "outputId": "fdfc8d85-6c2b-41b8-fbac-b0960eb4e6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature breakdown:\n",
            "Entity cohesion: 6\n",
            "Semantic cohesion: 10\n",
            "Topic coherence: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for missing values\n",
        "nan_counts = df_merged[all_features].isna().sum()\n",
        "nan_features = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(nan_features) > 0:\n",
        "    print(f\"  Found {len(nan_features)} features with missing values:\")\n",
        "    for feat, count in nan_features.items():\n",
        "        pct = 100 * count / len(df_merged)\n",
        "        print(f\"  {feat}: {count} ({pct:.2f}%)\")\n",
        "else:\n",
        "    print(\"No missing values\")\n",
        "\n",
        "# Check for infinite values\n",
        "print(\"Infinite Value Check:\")\n",
        "inf_counts = np.isinf(df_merged[all_features]).sum()\n",
        "inf_features = inf_counts[inf_counts > 0]\n",
        "\n",
        "if len(inf_features) > 0:\n",
        "    print(f\"  Found {len(inf_features)} features with infinite values:\")\n",
        "    for feat, count in inf_features.items():\n",
        "        print(f\"  {feat}: {count}\")\n",
        "else:\n",
        "    print(\"No infinite values found\")\n",
        "\n",
        "# Check value ranges for bounded features\n",
        "print(\"Value Range Validation:\")\n",
        "bounded_features = {\n",
        "    'entity_reuse_rate': (0, 1),\n",
        "    'entity_graph_density': (0, 1),\n",
        "    'entity_isolated_sentences': (0, 1),\n",
        "    'mean_entity_continuation_rate': (0, 1),\n",
        "    'entity_largest_component_size': (0, 1),\n",
        "    'topic_drift_rate': (0, 1),\n",
        "    'dominant_topic_proportion': (0, 1),\n",
        "    'topic_diversity': (0, 1),\n",
        "    'topic_concentration': (0, 1),\n",
        "    'topic_return_rate': (0, 1)\n",
        "}\n",
        "\n",
        "range_issues = []\n",
        "for feat, (min_val, max_val) in bounded_features.items():\n",
        "    if feat in df_merged.columns:\n",
        "        actual_min = df_merged[feat].min()\n",
        "        actual_max = df_merged[feat].max()\n",
        "\n",
        "        if actual_min < min_val or actual_max > max_val:\n",
        "            range_issues.append(feat)\n",
        "            print(f\"{feat}: [{actual_min:.4f}, {actual_max:.4f}] (expected [{min_val}, {max_val}])\")\n",
        "\n",
        "if not range_issues:\n",
        "    print(\"  All bounded features within expected ranges!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko1Cm2qR6awg",
        "outputId": "83d109e9-f9cc-446d-a300-4ae10d8d90db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 23 features with missing values:\n",
            "  topic_transition_similarity: 1317 (10.97%)\n",
            "  long_range_similarity: 1121 (9.34%)\n",
            "  topic_entropy: 952 (7.93%)\n",
            "  topic_diversity: 952 (7.93%)\n",
            "  dominant_topic_proportion: 952 (7.93%)\n",
            "  topic_concentration: 952 (7.93%)\n",
            "  mean_nonadjacent_similarity: 359 (2.99%)\n",
            "  entity_graph_density: 221 (1.84%)\n",
            "  mean_entity_continuation_rate: 221 (1.84%)\n",
            "  entity_largest_component_size: 221 (1.84%)\n",
            "  entity_isolated_sentences: 221 (1.84%)\n",
            "  entity_mention_density: 221 (1.84%)\n",
            "  entity_reuse_rate: 221 (1.84%)\n",
            "  mean_adjacent_cosine_similarity: 168 (1.40%)\n",
            "  min_adjacent_cosine_similarity: 168 (1.40%)\n",
            "  semantic_largest_component_size: 168 (1.40%)\n",
            "  semantic_graph_isolated_sentences: 168 (1.40%)\n",
            "  semantic_graph_density: 168 (1.40%)\n",
            "  similarity_decay_rate: 168 (1.40%)\n",
            "  adjacent_similarity_variance: 168 (1.40%)\n",
            "  semantic_average_degree: 168 (1.40%)\n",
            "  topic_drift_rate: 168 (1.40%)\n",
            "  topic_persistence: 168 (1.40%)\n",
            "Infinite Value Check:\n",
            "No infinite values found\n",
            "Value Range Validation:\n",
            "  All bounded features within expected ranges!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NAN IMPUTATION\")\n",
        "\n",
        "df_imputed = df_merged.copy()\n",
        "\n",
        "# Group 1: Density/rate features - impute with 0 (no entities/topics detected)\n",
        "density_rate_features = [\n",
        "    'entity_mention_density',\n",
        "    'entity_reuse_rate',\n",
        "    'entity_graph_density',\n",
        "    'mean_entity_continuation_rate',\n",
        "    'topic_drift_rate',\n",
        "    'topic_transition_similarity'\n",
        "]\n",
        "\n",
        "zero_imputed = []\n",
        "for feat in density_rate_features:\n",
        "    if feat in df_imputed.columns and df_imputed[feat].isna().any():\n",
        "        n_imputed = df_imputed[feat].isna().sum()\n",
        "        df_imputed[feat].fillna(0, inplace=True)\n",
        "        zero_imputed.append((feat, n_imputed))\n",
        "\n",
        "if zero_imputed:\n",
        "    print(\"  1. Zero imputation (no cohesion detected):\")\n",
        "    for feat, n in zero_imputed:\n",
        "        print(f\"   {feat}: {n} values\")\n",
        "\n",
        "#Proportion features - impute with 1.0\n",
        "proportion_features = [\n",
        "    'entity_isolated_sentences',\n",
        "    'dominant_topic_proportion'\n",
        "]\n",
        "\n",
        "one_imputed = []\n",
        "for feat in proportion_features:\n",
        "    if feat in df_imputed.columns and df_imputed[feat].isna().any():\n",
        "        n_imputed = df_imputed[feat].isna().sum()\n",
        "        df_imputed[feat].fillna(1.0, inplace=True)\n",
        "        one_imputed.append((feat, n))\n",
        "\n",
        "if one_imputed:\n",
        "    print(\"One imputation (complete isolation/dominance)\")\n",
        "    for feat, n in one_imputed:\n",
        "        print(f\"   {feat}: {n} values\")\n",
        "\n",
        "# Group 3: Count features - impute with 0\n",
        "count_features = [\n",
        "    'topic_switching_frequency',\n",
        "    'num_distinct_topics'\n",
        "]\n",
        "\n",
        "count_imputed = []\n",
        "for feat in count_features:\n",
        "    if feat in df_imputed.columns and df_imputed[feat].isna().any():\n",
        "        n_imputed = df_imputed[feat].isna().sum()\n",
        "        df_imputed[feat].fillna(0, inplace=True)\n",
        "        count_imputed.append((feat, n))\n",
        "\n",
        "if count_imputed:\n",
        "    print(\"Zero imputation:\")\n",
        "    for feat, n in count_imputed:\n",
        "        print(f\"   {feat}: {n} values\")\n",
        "\n",
        "#Remaining features - impute with median\n",
        "median_imputed = []\n",
        "for feat in all_features:\n",
        "    if feat in df_imputed.columns and df_imputed[feat].isna().any():\n",
        "        n_imputed = df_imputed[feat].isna().sum()\n",
        "        median_val = df_imputed[feat].median()\n",
        "        df_imputed[feat].fillna(median_val, inplace=True)\n",
        "        median_imputed.append((feat, n, median_val))\n",
        "\n",
        "if median_imputed:\n",
        "    print(\"  4. Median imputation (remaining features):\")\n",
        "    for feat, n, med in median_imputed:\n",
        "        print(f\"   {feat}: {n} values (median={med:.4f})\")\n",
        "\n",
        "# Verify no NaNs remain\n",
        "remaining_nans = df_imputed[all_features].isna().sum().sum()\n",
        "print(f\"\\nImputation complete: {remaining_nans} NaN values remaining\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "873NWYAb6eto",
        "outputId": "d41ee1fd-644f-4c6f-9c04-f95ec954f8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAN IMPUTATION\n",
            "  1. Zero imputation (no cohesion detected):\n",
            "   entity_mention_density: 221 values\n",
            "   entity_reuse_rate: 221 values\n",
            "   entity_graph_density: 221 values\n",
            "   mean_entity_continuation_rate: 221 values\n",
            "   topic_drift_rate: 168 values\n",
            "   topic_transition_similarity: 1317 values\n",
            "One imputation (complete isolation/dominance)\n",
            "   entity_isolated_sentences: 1317 values\n",
            "   dominant_topic_proportion: 1317 values\n",
            "  4. Median imputation (remaining features):\n",
            "   entity_largest_component_size: 1317 values (median=0.7500)\n",
            "   mean_adjacent_cosine_similarity: 1317 values (median=0.3644)\n",
            "   min_adjacent_cosine_similarity: 1317 values (median=0.1404)\n",
            "   adjacent_similarity_variance: 1317 values (median=0.0179)\n",
            "   semantic_graph_density: 1317 values (median=0.0476)\n",
            "   similarity_decay_rate: 1317 values (median=-0.0022)\n",
            "   mean_nonadjacent_similarity: 1317 values (median=0.3369)\n",
            "   long_range_similarity: 1317 values (median=0.3098)\n",
            "   semantic_graph_isolated_sentences: 1317 values (median=0.6098)\n",
            "   semantic_largest_component_size: 1317 values (median=0.2857)\n",
            "   semantic_average_degree: 1317 values (median=0.5000)\n",
            "   topic_entropy: 1317 values (median=0.0000)\n",
            "   topic_persistence: 1317 values (median=2.0000)\n",
            "   topic_diversity: 1317 values (median=0.0000)\n",
            "   topic_concentration: 1317 values (median=0.0000)\n",
            "\n",
            "Imputation complete: 0 NaN values remaining\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#REDUNDANCY ELIMINATION\n",
        "\n",
        "correlation_matrix = df_imputed[all_features].corr().abs()\n",
        "\n",
        "# Find highly correlated pairs (> 0.95)\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "high_corr_pairs = []\n",
        "threshold = 0.95\n",
        "\n",
        "for column in upper_triangle.columns:\n",
        "    high_corr = upper_triangle[column][upper_triangle[column] > threshold]\n",
        "    for idx in high_corr.index:\n",
        "        high_corr_pairs.append((column, idx, upper_triangle.loc[idx, column]))\n",
        "\n",
        "print(f\"Found {len(high_corr_pairs)} highly correlated pairs (r > {threshold}):\")\n",
        "\n",
        "if high_corr_pairs:\n",
        "    redundant_features = set()\n",
        "\n",
        "    for feat1, feat2, corr_val in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True):\n",
        "        print(f\"{feat1} <-> {feat2}: r={corr_val:.4f}\")\n",
        "\n",
        "        # Keep the feature with lower mean absolute correlation to all others\n",
        "        avg_corr_1 = correlation_matrix[feat1].abs().mean()\n",
        "        avg_corr_2 = correlation_matrix[feat2].abs().mean()\n",
        "\n",
        "        if avg_corr_1 > avg_corr_2:\n",
        "            redundant_features.add(feat1)\n",
        "            print(f\"    → Removing {feat1} (avg_corr={avg_corr_1:.3f} > {avg_corr_2:.3f})\")\n",
        "        else:\n",
        "            redundant_features.add(feat2)\n",
        "            print(f\"    → Removing {feat2} (avg_corr={avg_corr_2:.3f} > {avg_corr_1:.3f})\")\n",
        "\n",
        "    #Remove redundant features\n",
        "    features_to_keep = [f for f in all_features if f not in redundant_features]\n",
        "\n",
        "    print(f\"\\nRemoved {len(redundant_features)} redundant features\")\n",
        "    print(f\"  Remaining: {len(features_to_keep)} features\")\n",
        "\n",
        "else:\n",
        "    print(\"  No highly correlated feature pairs found!\")\n",
        "    features_to_keep = all_features.copy()\n",
        "\n",
        "# Update feature lists after redundancy removal\n",
        "backbone_features_kept = [f for f in backbone_features if f in features_to_keep]\n",
        "entity_features_kept = [f for f in entity_features if f in features_to_keep]\n",
        "semantic_features_kept = [f for f in semantic_features if f in features_to_keep]\n",
        "topic_features_kept = [f for f in topic_features if f in features_to_keep]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6udUOB_N6fKC",
        "outputId": "56734b1e-ae26-458a-9dca-ce0470ba02fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 highly correlated pairs (r > 0.95):\n",
            "topic_diversity <-> topic_entropy: r=0.9784\n",
            "    → Removing topic_entropy (avg_corr=0.251 > 0.250)\n",
            "dominant_topic_proportion <-> topic_entropy: r=0.9634\n",
            "    → Removing topic_entropy (avg_corr=0.251 > 0.231)\n",
            "topic_diversity <-> dominant_topic_proportion: r=0.9570\n",
            "    → Removing topic_diversity (avg_corr=0.250 > 0.231)\n",
            "\n",
            "Removed 2 redundant features\n",
            "  Remaining: 33 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "# Separate AI and Human samples\n",
        "df_ai = df_imputed[df_imputed['is_ai'] == 1]\n",
        "df_human = df_imputed[df_imputed['is_ai'] == 0]\n",
        "\n",
        "# Compute descriptive statistics by class\n",
        "print(\"Computing descriptive statistics...\")\n",
        "\n",
        "descriptive_stats = []\n",
        "\n",
        "for feature in features_to_keep:\n",
        "    ai_values = df_ai[feature]\n",
        "    human_values = df_human[feature]\n",
        "\n",
        "    descriptive_stats.append({\n",
        "        'feature': feature,\n",
        "        'ai_mean': ai_values.mean(),\n",
        "        'ai_std': ai_values.std(),\n",
        "        'ai_median': ai_values.median(),\n",
        "        'human_mean': human_values.mean(),\n",
        "        'human_std': human_values.std(),\n",
        "        'human_median': human_values.median(),\n",
        "        'diff_mean': ai_values.mean() - human_values.mean(),\n",
        "        'diff_pct': 100 * (ai_values.mean() - human_values.mean()) / human_values.mean() if human_values.mean() != 0 else np.nan\n",
        "    })\n",
        "\n",
        "df_stats = pd.DataFrame(descriptive_stats)\n",
        "\n",
        "# Save descriptive statistics\n",
        "stats_path = os.path.join(OUTPUT_DIR, 'tables', 'descriptive_statistics.csv')\n",
        "df_stats.to_csv(stats_path, index=False)\n",
        "print(f\"Descriptive statistics saved: {stats_path}\")\n",
        "\n",
        "# Display top differences\n",
        "print(\"Top 10 features by absolute mean difference:\")\n",
        "df_stats_sorted = df_stats.sort_values('diff_mean', key=abs, ascending=False)\n",
        "print(df_stats_sorted[['feature', 'ai_mean', 'human_mean', 'diff_mean', 'diff_pct']].head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8drJ8qQ_6jCd",
        "outputId": "1b862406-e298-41ba-e323-dfe8aa059d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing descriptive statistics...\n",
            "Descriptive statistics saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/tables/descriptive_statistics.csv\n",
            "Top 10 features by absolute mean difference:\n",
            "                          feature    ai_mean  human_mean  diff_mean   diff_pct\n",
            "                          yules_k 155.689689  126.320699  29.368990  23.249547\n",
            "           entity_mention_density 474.369990  463.723917  10.646073   2.295778\n",
            "        topic_switching_frequency   5.032500    6.019333  -0.986833 -16.394396\n",
            "          semantic_average_degree   2.155866    1.522370   0.633496  41.612486\n",
            "              sentence_length_std   9.652472   10.275792  -0.623320  -6.065907\n",
            "                   avg_tree_depth   5.875135    5.568922   0.306213   5.498610\n",
            "                topic_persistence   2.842883    2.563553   0.279330  10.896213\n",
            "                verbs_per_100_tok  14.334342   14.608463  -0.274121  -1.876456\n",
            "             char_trigram_entropy   8.791704    8.967907  -0.176203  -1.964818\n",
            "semantic_graph_isolated_sentences   0.535066    0.640233  -0.105167 -16.426413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution Comparisons (Feature Groups)\n",
        "\n",
        "\n",
        "print(\"Generating distribution comparison plots...\")\n",
        "\n",
        "feature_groups = {\n",
        "    'Entity Cohesion': entity_features_kept,\n",
        "    'Semantic Cohesion': semantic_features_kept,\n",
        "    'Topic Coherence': topic_features_kept\n",
        "}\n",
        "\n",
        "for group_name, group_features in feature_groups.items():\n",
        "    if len(group_features) == 0:\n",
        "        continue\n",
        "\n",
        "    n_features = len(group_features)\n",
        "    n_cols = 3\n",
        "    n_rows = int(np.ceil(n_features / n_cols))\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "\n",
        "    for idx, feature in enumerate(group_features):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Plot distributions\n",
        "        ai_vals = df_ai[feature].dropna()\n",
        "        human_vals = df_human[feature].dropna()\n",
        "\n",
        "        ax.hist(human_vals, bins=30, alpha=0.5, label='Human', density=True, color='blue')\n",
        "        ax.hist(ai_vals, bins=30, alpha=0.5, label='AI', density=True, color='red')\n",
        "\n",
        "        ax.set_xlabel(feature, fontsize=10)\n",
        "        ax.set_ylabel('Density', fontsize=10)\n",
        "        ax.set_title(f'{feature}\\n(AI: {ai_vals.mean():.3f}, Human: {human_vals.mean():.3f})',\n",
        "                     fontsize=10)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_features, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(OUTPUT_DIR, 'plots', f'distributions_{group_name.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"  Saved: {plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q-0_tjs6l1o",
        "outputId": "3c1fb280-53fa-4fdb-b93c-75a61bc7041e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating distribution comparison plots...\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/distributions_entity_cohesion.png\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/distributions_semantic_cohesion.png\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/distributions_topic_coherence.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#STATISTICAL TESTING\n",
        "\n",
        "def cohens_d(group1, group2):\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1, var2 = group1.var(), group2.var()\n",
        "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
        "    return (group1.mean() - group2.mean()) / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "statistical_results = []\n",
        "\n",
        "for feature in features_to_keep:\n",
        "    ai_vals = df_ai[feature].dropna()\n",
        "    human_vals = df_human[feature].dropna()\n",
        "\n",
        "    # T-test\n",
        "    t_stat, p_value = ttest_ind(ai_vals, human_vals, equal_var=False)\n",
        "\n",
        "    # Cohen's d\n",
        "    d = cohens_d(ai_vals, human_vals)\n",
        "\n",
        "    # Effect size interpretation\n",
        "    if abs(d) < 0.2:\n",
        "        effect_size = 'negligible'\n",
        "    elif abs(d) < 0.5:\n",
        "        effect_size = 'small'\n",
        "    elif abs(d) < 0.8:\n",
        "        effect_size = 'medium'\n",
        "    else:\n",
        "        effect_size = 'large'\n",
        "\n",
        "    statistical_results.append({\n",
        "        'feature': feature,\n",
        "        't_statistic': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'cohens_d': d,\n",
        "        'effect_size': effect_size,\n",
        "        'significant': p_value < 0.05,\n",
        "        'ai_mean': ai_vals.mean(),\n",
        "        'human_mean': human_vals.mean()\n",
        "    })\n",
        "\n",
        "df_stats_tests = pd.DataFrame(statistical_results)\n",
        "\n",
        "# Apply Bonferroni correction\n",
        "bonferroni_threshold = 0.05 / len(features_to_keep)\n",
        "df_stats_tests['bonferroni_significant'] = df_stats_tests['p_value'] < bonferroni_threshold\n",
        "\n",
        "# Save statistical results\n",
        "stats_test_path = os.path.join(OUTPUT_DIR, 'tables', 'statistical_tests.csv')\n",
        "df_stats_tests.to_csv(stats_test_path, index=False)\n",
        "print(f\"Statistical test results saved: {stats_test_path}\")\n",
        "\n",
        "# Summary statistics\n",
        "n_significant = (df_stats_tests['p_value'] < 0.05).sum()\n",
        "n_bonferroni = df_stats_tests['bonferroni_significant'].sum()\n",
        "\n",
        "print(f\"Significant (p < 0.05): {n_significant}/{len(features_to_keep)} ({100*n_significant/len(features_to_keep):.1f}%)\")\n",
        "\n",
        "# Effect size distribution\n",
        "print(f\"Effect size distribution:\")\n",
        "for effect in ['negligible', 'small', 'medium', 'large']:\n",
        "    count = (df_stats_tests['effect_size'] == effect).sum()\n",
        "    print(f\"{effect.capitalize()}: {count} ({100*count/len(df_stats_tests):.1f}%)\")\n",
        "\n",
        "# Top 15 features by effect size\n",
        "print(\"Top 15 features by |Cohen's d|:\")\n",
        "df_top_effects = df_stats_tests.sort_values('cohens_d', key=abs, ascending=False).head(15)\n",
        "print(df_top_effects[['feature', 'cohens_d', 'p_value', 'effect_size']].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4tR4ccM6tW8",
        "outputId": "a9e408fe-13ba-410c-b2c4-c6b5ad005d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical test results saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/tables/statistical_tests.csv\n",
            "Significant (p < 0.05): 28/33 (84.8%)\n",
            "Effect size distribution:\n",
            "Negligible: 25 (75.8%)\n",
            "Small: 8 (24.2%)\n",
            "Medium: 0 (0.0%)\n",
            "Large: 0 (0.0%)\n",
            "Top 15 features by |Cohen's d|:\n",
            "                          feature  cohens_d      p_value effect_size\n",
            "                trigram_diversity -0.382753 1.175277e-94       small\n",
            "  semantic_largest_component_size  0.326511 1.307571e-70       small\n",
            "semantic_graph_isolated_sentences -0.326179 1.809167e-70       small\n",
            "             char_trigram_entropy -0.297851 3.986830e-59       small\n",
            "      mean_nonadjacent_similarity  0.281777 3.273303e-53       small\n",
            "                          yules_k  0.268598 2.326385e-48       small\n",
            "            long_range_similarity  0.258202 4.896273e-45       small\n",
            "           semantic_graph_density  0.246637 2.828498e-41       small\n",
            "                   avg_tree_depth  0.175431 8.856409e-22  negligible\n",
            "   min_adjacent_cosine_similarity  0.169791 1.652349e-20  negligible\n",
            "  mean_adjacent_cosine_similarity  0.165425 1.503003e-19  negligible\n",
            "        entity_isolated_sentences  0.153931 3.820831e-17  negligible\n",
            "        topic_switching_frequency -0.150309 2.060814e-16  negligible\n",
            "                      comma_ratio  0.135139 1.437450e-13  negligible\n",
            "          semantic_average_degree  0.128497 2.058361e-12  negligible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VISUALIZATION 2: Effect Size Plot\n",
        "\n",
        "\n",
        "print(\"Generating effect size visualization...\")\n",
        "\n",
        "# Sort by Cohen's d\n",
        "df_plot = df_stats_tests.sort_values('cohens_d', ascending=True)\n",
        "\n",
        "# Color by significance\n",
        "colors = ['red' if sig else 'gray' for sig in df_plot['bonferroni_significant']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(8, len(features_to_keep) * 0.3)))\n",
        "\n",
        "y_pos = np.arange(len(df_plot))\n",
        "ax.barh(y_pos, df_plot['cohens_d'], color=colors, alpha=0.7)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot['feature'], fontsize=8)\n",
        "ax.set_xlabel(\"Cohen's d (AI - Human)\", fontsize=12, fontweight='bold')\n",
        "ax.set_title(\"Effect Sizes: All Cohesion Features\",\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "ax.axvline(x=-0.2, color='blue', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.axvline(x=0.2, color='blue', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.axvline(x=-0.5, color='orange', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.axvline(x=0.5, color='orange', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "effect_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'cohens_d_all_features.png')\n",
        "plt.savefig(effect_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  Saved: {effect_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuICKv0r6v6f",
        "outputId": "1d16f752-a95c-4e20-a15b-8e8bd0b4be4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating effect size visualization...\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/cohens_d_all_features.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#XGBOOST FEATURE IMPORTANCE\n",
        "\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "X = df_imputed[features_to_keep].values\n",
        "y = df_imputed['is_ai'].values\n",
        "\n",
        "print(f\"Class balance: {(y==1).mean()} AI / {(y==0).mean()} Human\")\n",
        "\n",
        "# Train XGBoost with optimal hyperparameters\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=9,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='f1')\n",
        "print(f\"F1 scores: {cv_scores}\")\n",
        "print(f\"Mean F1: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
        "\n",
        "# Train final model\n",
        "print(\"Training final model on full dataset...\")\n",
        "xgb_model.fit(X, y)\n",
        "print(\"  Training complete\")\n",
        "\n",
        "# Extract feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': features_to_keep,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Normalize to sum to 1\n",
        "feature_importance['importance_pct'] = 100 * feature_importance['importance'] / feature_importance['importance'].sum()\n",
        "\n",
        "# Save feature importance\n",
        "importance_path = os.path.join(OUTPUT_DIR, 'tables', 'xgboost_feature_importance.csv')\n",
        "feature_importance.to_csv(importance_path, index=False)\n",
        "print(f\"Feature importance saved: {importance_path}\")\n",
        "\n",
        "print(\"Top 20 features by XGBoost importance:\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "# VISUALIZATION 3: XGBoost Feature Importance\n",
        "\n",
        "\n",
        "# Top 30 features\n",
        "top_n = min(30, len(feature_importance))\n",
        "df_plot_importance = feature_importance.head(top_n).sort_values('importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(8, top_n * 0.3)))\n",
        "\n",
        "y_pos = np.arange(len(df_plot_importance))\n",
        "ax.barh(y_pos, df_plot_importance['importance'], color='steelblue', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(df_plot_importance['feature'], fontsize=9)\n",
        "ax.set_xlabel('XGBoost Feature Importance', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Top {top_n} Features by XGBoost Importance\\n(F1={cv_scores.mean():.4f})',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "importance_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'xgboost_importance_top30.png')\n",
        "plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  Saved: {importance_plot_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbXclhbt6xz0",
        "outputId": "b46cf2a5-22ad-4682-93ce-45a3ccbcfafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class balance: 0.5 AI / 0.5 Human\n",
            "F1 scores: [0.84487888 0.85498743 0.85279188 0.85579399 0.86243164]\n",
            "Mean F1: 0.8542 (±0.0057)\n",
            "Training final model on full dataset...\n",
            "  Training complete\n",
            "Feature importance saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/tables/xgboost_feature_importance.csv\n",
            "Top 20 features by XGBoost importance:\n",
            "                          feature  importance  importance_pct\n",
            "                          yules_k    0.075597        7.559721\n",
            "          semantic_average_degree    0.063307        6.330723\n",
            "        dominant_topic_proportion    0.059080        5.908017\n",
            "              num_distinct_topics    0.053578        5.357793\n",
            "              sentence_length_std    0.047956        4.795584\n",
            "                trigram_diversity    0.047447        4.744661\n",
            "                 type_token_ratio    0.042047        4.204652\n",
            "             char_trigram_entropy    0.040805        4.080469\n",
            "semantic_graph_isolated_sentences    0.038773        3.877320\n",
            "           semantic_graph_density    0.035495        3.549506\n",
            "           content_function_ratio    0.033888        3.388824\n",
            "                      comma_ratio    0.032068        3.206827\n",
            "                verbs_per_100_tok    0.028272        2.827238\n",
            "        topic_switching_frequency    0.025961        2.596083\n",
            "  mean_adjacent_cosine_similarity    0.025414        2.541433\n",
            "              topic_concentration    0.023649        2.364929\n",
            "            long_range_similarity    0.022978        2.297822\n",
            "      mean_nonadjacent_similarity    0.022636        2.263596\n",
            "           entity_mention_density    0.021690        2.168954\n",
            "                   avg_tree_depth    0.020723        2.072303\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/xgboost_importance_top30.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VISUALIZATION 4: Feature Group Contributions\n",
        "\n",
        "\n",
        "# Aggregate importance by feature group\n",
        "group_contributions = []\n",
        "\n",
        "for group_name, group_features in feature_groups.items():\n",
        "    group_features_kept = [f for f in group_features if f in features_to_keep]\n",
        "    if len(group_features_kept) > 0:\n",
        "        group_importance = feature_importance[feature_importance['feature'].isin(group_features_kept)]['importance'].sum()\n",
        "        group_contributions.append({\n",
        "            'group': group_name,\n",
        "            'total_importance': group_importance,\n",
        "            'importance_pct': 100 * group_importance / feature_importance['importance'].sum(),\n",
        "            'importance_per_feature': group_importance / len(group_features_kept)\n",
        "        })\n",
        "\n",
        "df_group_contrib = pd.DataFrame(group_contributions).sort_values('total_importance', ascending=False)\n",
        "\n",
        "print(\"Feature group contributions:\")\n",
        "print(df_group_contrib.to_string(index=False))\n",
        "\n",
        "# Save group contributions\n",
        "group_contrib_path = os.path.join(OUTPUT_DIR, 'tables', 'feature_group_contributions.csv')\n",
        "df_group_contrib.to_csv(group_contrib_path, index=False)\n",
        "\n",
        "# Plot group contributions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Total importance by group\n",
        "ax1.barh(df_group_contrib['group'], df_group_contrib['importance_pct'], color='teal', alpha=0.7)\n",
        "ax1.set_xlabel('Total Importance (%)', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Feature Group Contributions\\n(Total Importance)', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Importance per feature\n",
        "ax2.barh(df_group_contrib['group'], df_group_contrib['importance_per_feature'], color='coral', alpha=0.7)\n",
        "ax2.set_xlabel('Average Importance per Feature', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('Feature Group Contributions\\n(Per-Feature Average)', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "group_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'feature_group_contributions.png')\n",
        "plt.savefig(group_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  Saved: {group_plot_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkj7U-jq6zo1",
        "outputId": "250fbe78-b649-4f8a-956a-160d2627fced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature group contributions:\n",
            "            group  total_importance  importance_pct  importance_per_feature\n",
            "Semantic Cohesion          0.282049       28.204950                0.028205\n",
            "  Topic Coherence          0.233105       23.310482                0.029138\n",
            "  Entity Cohesion          0.116043       11.604295                0.019340\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/feature_group_contributions.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZATION 5: Correlation Heatmap\n",
        "\n",
        "# Select top 20 features by importance\n",
        "top_features = feature_importance.head(20)['feature'].tolist()\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df_imputed[top_features].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "            vmin=-1, vmax=1, ax=ax)\n",
        "\n",
        "ax.set_title('Correlation Matrix: Top 20 Features by XGBoost Importance',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "heatmap_path = os.path.join(OUTPUT_DIR, 'plots', 'correlation_heatmap_top20.png')\n",
        "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  Saved: {heatmap_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQynRN_D61Su",
        "outputId": "5a9386be-0a31-4cac-9753-e8181cae3464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/correlation_heatmap_top20.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_backbone = df_imputed[backbone_features_kept].values\n",
        "y_target = df_imputed['is_ai'].values\n",
        "xgb_backbone = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=9,\n",
        "    learning_rate=0.15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "backbone_scores = cross_val_score(xgb_backbone, X_backbone, y_target, cv=5, scoring='f1')\n",
        "backbone_f1 = backbone_scores.mean()\n",
        "\n",
        "print(f\"Backbone-only F1: {backbone_f1:.4f} (±{backbone_scores.std():.4f})\")\n",
        "\n",
        "# Test each cohesion feature individually\n",
        "individual_results = []\n",
        "\n",
        "all_cohesion = entity_features_kept + semantic_features_kept + topic_features_kept\n",
        "\n",
        "print(f\"Testing {len(all_cohesion)} cohesion features individually...\")\n",
        "\n",
        "for cohesion_feat in tqdm(all_cohesion, desc=\"Testing features\"):\n",
        "    features_test = backbone_features_kept + [cohesion_feat]\n",
        "    X_test = df_imputed[features_test].values\n",
        "\n",
        "    xgb_test = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=9,\n",
        "        learning_rate=0.15,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    test_scores = cross_val_score(xgb_test, X_test, y_target, cv=5, scoring='f1')\n",
        "    test_f1 = test_scores.mean()\n",
        "\n",
        "    # Calculate improvement\n",
        "    improvement = test_f1 - backbone_f1\n",
        "    improvement_pct = 100 * improvement / backbone_f1\n",
        "\n",
        "    # Determine feature group\n",
        "    if cohesion_feat in entity_features_kept:\n",
        "        group = 'Entity Cohesion'\n",
        "    elif cohesion_feat in semantic_features_kept:\n",
        "        group = 'Semantic Cohesion'\n",
        "    elif cohesion_feat in topic_features_kept:\n",
        "        group = 'Topic Coherence'\n",
        "    else:\n",
        "        group = 'Unknown'\n",
        "\n",
        "    individual_results.append({\n",
        "        'feature': cohesion_feat,\n",
        "        'group': group,\n",
        "        'backbone_f1': backbone_f1,\n",
        "        'with_feature_f1': test_f1,\n",
        "        'improvement': improvement,\n",
        "        'improvement_pct': improvement_pct,\n",
        "        'std': test_scores.std()\n",
        "    })\n",
        "\n",
        "df_individual = pd.DataFrame(individual_results).sort_values('improvement', ascending=False)\n",
        "\n",
        "# Save results\n",
        "individual_path = os.path.join(OUTPUT_DIR, 'tables', 'individual_feature_contributions.csv')\n",
        "df_individual.to_csv(individual_path, index=False)\n",
        "print(f\"\\nIndividual feature results saved: {individual_path}\")\n",
        "\n",
        "# Analyze results\n",
        "positive_improvements = df_individual[df_individual['improvement'] > 0]\n",
        "negative_improvements = df_individual[df_individual['improvement'] <= 0]\n",
        "\n",
        "print(f\"Individual Feature Analysis:\")\n",
        "print(f\"Features with positive impact: {len(positive_improvements)} ({100*len(positive_improvements)/len(df_individual):.1f}%)\")\n",
        "print(f\"Features with negative/neutral impact: {len(negative_improvements)} ({100*len(negative_improvements)/len(df_individual):.1f}%)\")\n",
        "print(f\"Best individual feature: {df_individual.iloc[0]['feature']} (+{df_individual.iloc[0]['improvement']:.4f})\")\n",
        "print(f\"Worst individual feature: {df_individual.iloc[-1]['feature']} ({df_individual.iloc[-1]['improvement']:.4f})\")\n",
        "\n",
        "print(\"Top 15 Features to Add to Backbone:\")\n",
        "print(df_individual.head(15)[['feature', 'group', 'improvement', 'improvement_pct']].to_string(index=False))\n",
        "\n",
        "print(\"Bottom 10 Features (Avoid Adding):\")\n",
        "print(df_individual.tail(10)[['feature', 'group', 'improvement', 'improvement_pct']].to_string(index=False))\n",
        "\n",
        "# Group-level summary\n",
        "print(\"Group-Level Performance (Individual Additions):\")\n",
        "group_summary = df_individual.groupby('group').agg({\n",
        "    'improvement': ['mean', 'std', 'min', 'max', 'count'],\n",
        "}).round(4)\n",
        "group_summary.columns = ['Mean Δ', 'Std Δ', 'Min Δ', 'Max Δ', 'N Features']\n",
        "print(group_summary.to_string())\n",
        "\n",
        "\n",
        "# VISUALIZATION 1: Individual Feature Contributions\n",
        "\n",
        "\n",
        "print(\"Creating individual feature contribution visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "# Define colors\n",
        "group_colors = {\n",
        "    'Entity Cohesion': '#ff7f0e',\n",
        "    'Semantic Cohesion': '#2ca02c',\n",
        "    'Topic Coherence': '#d62728'\n",
        "}\n",
        "\n",
        "# Plot 1: All features sorted by improvement\n",
        "ax1 = axes[0, 0]\n",
        "colors_all = [group_colors[g] for g in df_individual['group']]\n",
        "y_pos = np.arange(len(df_individual))\n",
        "ax1.barh(y_pos, df_individual['improvement'], color=colors_all, alpha=0.7)\n",
        "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax1.set_xlabel('F1 Improvement over Backbone', fontsize=11, fontweight='bold')\n",
        "ax1.set_ylabel('Feature Index (sorted by improvement)', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('A) All Features: Individual Contribution to Backbone\\n(Sorted by F1 Improvement)',\n",
        "              fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 2: Top 20 features\n",
        "ax2 = axes[0, 1]\n",
        "top_20 = df_individual.head(20).sort_values('improvement', ascending=True)\n",
        "colors_top20 = [group_colors[g] for g in top_20['group']]\n",
        "y_pos_top20 = np.arange(len(top_20))\n",
        "ax2.barh(y_pos_top20, top_20['improvement'], color=colors_top20, alpha=0.85)\n",
        "ax2.set_yticks(y_pos_top20)\n",
        "ax2.set_yticklabels(top_20['feature'], fontsize=9)\n",
        "ax2.set_xlabel('F1 Improvement', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('B) Top 20 Features Worth Adding\\n(Positive Impact)',\n",
        "              fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='x')\n",
        "# Add value labels\n",
        "for i, (yval, val) in enumerate(zip(y_pos_top20, top_20['improvement'])):\n",
        "    ax2.text(val + 0.0001, yval, f'{val:.4f}', va='center', fontsize=8)\n",
        "\n",
        "# Plot 3: Bottom 20 features\n",
        "ax3 = axes[1, 0]\n",
        "bottom_20 = df_individual.tail(20).sort_values('improvement', ascending=True)\n",
        "colors_bottom20 = [group_colors[g] for g in bottom_20['group']]\n",
        "y_pos_bottom20 = np.arange(len(bottom_20))\n",
        "ax3.barh(y_pos_bottom20, bottom_20['improvement'], color=colors_bottom20, alpha=0.85)\n",
        "ax3.set_yticks(y_pos_bottom20)\n",
        "ax3.set_yticklabels(bottom_20['feature'], fontsize=9)\n",
        "ax3.set_xlabel('F1 Improvement', fontsize=11, fontweight='bold')\n",
        "ax3.set_title('C) Bottom 20 Features to Avoid\\n(Negative/Neutral Impact)',\n",
        "              fontsize=12, fontweight='bold')\n",
        "ax3.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 4: Group comparison (box plot)\n",
        "ax4 = axes[1, 1]\n",
        "group_data = [df_individual[df_individual['group'] == g]['improvement'].values\n",
        "              for g in ['Entity Cohesion', 'Semantic Cohesion', 'Topic Coherence']]\n",
        "bp = ax4.boxplot(group_data, labels=['Entity\\nCohesion', 'Semantic\\nCohesion', 'Topic\\nCoherence'],\n",
        "                  patch_artist=True, showmeans=True)\n",
        "for patch, color in zip(bp['boxes'], [group_colors[g] for g in ['Entity Cohesion', 'Semantic Cohesion', 'Topic Coherence']]):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "ax4.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax4.set_ylabel('F1 Improvement', fontsize=11, fontweight='bold')\n",
        "ax4.set_title('D) Feature Group Distribution\\n(Individual Contributions)',\n",
        "              fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle(f'Individual Feature Contributions to Backbone (F1={backbone_f1:.4f})\\nWhich Features Should You Add?',\n",
        "             fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "individual_plot_path = os.path.join(OUTPUT_DIR, 'plots', 'individual_feature_contributions.png')\n",
        "plt.savefig(individual_plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  Saved: {individual_plot_path}\")\n",
        "\n",
        "\n",
        "# PROGRESSIVE FEATURE ADDITION\n",
        "\n",
        "\n",
        "print(\"Building optimal feature set progressively...\")\n",
        "\n",
        "# Start with backbone\n",
        "current_features = backbone_features_kept.copy()\n",
        "remaining_features = all_cohesion.copy()\n",
        "progressive_results = []\n",
        "\n",
        "# Initial baseline\n",
        "progressive_results.append({\n",
        "    'step': 0,\n",
        "    'feature_added': 'Backbone Only',\n",
        "    'f1_mean': backbone_f1,\n",
        "    'f1_std': backbone_scores.std(),\n",
        "    'improvement': 0.0\n",
        "})\n",
        "\n",
        "print(f\"\\n  Step 0: Backbone Only\")\n",
        "print(f\"    F1: {backbone_f1:.4f}\")\n",
        "\n",
        "\n",
        "MAX_ADDITIONS = 20\n",
        "\n",
        "for step in range(1, MAX_ADDITIONS + 1):\n",
        "    print(f\"\\n  Step {step}: Testing {len(remaining_features)} remaining features...\")\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_feature = None\n",
        "    best_std = 0\n",
        "\n",
        "    # Test each remaining feature\n",
        "    for feat in remaining_features:\n",
        "        test_features = current_features + [feat]\n",
        "        X_test = df_imputed[test_features].values\n",
        "\n",
        "        xgb_test = XGBClassifier(\n",
        "            n_estimators=500,\n",
        "            max_depth=9,\n",
        "            learning_rate=0.15,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "\n",
        "        test_scores = cross_val_score(xgb_test, X_test, y_target, cv=5, scoring='f1')  # FIXED: using y_target\n",
        "        test_f1 = test_scores.mean()\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_feature = feat\n",
        "            best_std = test_scores.std()\n",
        "\n",
        "    # Add best feature to current set\n",
        "    current_features.append(best_feature)\n",
        "    remaining_features.remove(best_feature)\n",
        "\n",
        "    # Calculate improvement\n",
        "    prev_f1 = progressive_results[-1]['f1_mean']\n",
        "    improvement = best_f1 - prev_f1\n",
        "\n",
        "    progressive_results.append({\n",
        "        'step': step,\n",
        "        'feature_added': best_feature,\n",
        "        'f1_mean': best_f1,\n",
        "        'f1_std': best_std,\n",
        "        'improvement': improvement\n",
        "    })\n",
        "\n",
        "    print(f\"    Best feature: {best_feature}\")\n",
        "    print(f\"    F1: {best_f1:.4f} (±{best_std:.4f})\")\n",
        "    print(f\"    Improvement: +{improvement:.4f}\")\n",
        "\n",
        "    # Stop if improvement is negligible\n",
        "    if improvement < 0.0001:\n",
        "        print(f\"    → Stopping: improvement below threshold\")\n",
        "        break\n",
        "\n",
        "df_progressive = pd.DataFrame(progressive_results)\n",
        "\n",
        "# Save results\n",
        "progressive_path = os.path.join(OUTPUT_DIR, 'tables', 'progressive_feature_selection.csv')\n",
        "df_progressive.to_csv(progressive_path, index=False)\n",
        "print(f\"\\nProgressive selection results saved: {progressive_path}\")\n",
        "\n",
        "print(\"Progressive Addition Results:\")\n",
        "print(df_progressive[['step', 'feature_added','f1_mean', 'improvement']].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pp1Li5nhM07u",
        "outputId": "a078ad32-23ee-4f52-ed94-6eaf42635e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone-only F1: 0.8355 (±0.0117)\n",
            "Testing 24 cohesion features individually...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing features: 100%|██████████| 24/24 [03:54<00:00,  9.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Individual feature results saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/tables/individual_feature_contributions.csv\n",
            "Individual Feature Analysis:\n",
            "Features with positive impact: 10 (41.7%)\n",
            "Features with negative/neutral impact: 14 (58.3%)\n",
            "Best individual feature: mean_nonadjacent_similarity (+0.0048)\n",
            "Worst individual feature: topic_return_rate (-0.0032)\n",
            "Top 15 Features to Add to Backbone:\n",
            "                          feature             group  improvement  improvement_pct\n",
            "      mean_nonadjacent_similarity Semantic Cohesion     0.004845         0.579926\n",
            "semantic_graph_isolated_sentences Semantic Cohesion     0.004608         0.551536\n",
            "          semantic_average_degree Semantic Cohesion     0.004469         0.534844\n",
            "           semantic_graph_density Semantic Cohesion     0.003888         0.465291\n",
            "        dominant_topic_proportion   Topic Coherence     0.003345         0.400417\n",
            "  mean_adjacent_cosine_similarity Semantic Cohesion     0.002367         0.283334\n",
            "  semantic_largest_component_size Semantic Cohesion     0.001505         0.180151\n",
            "   min_adjacent_cosine_similarity Semantic Cohesion     0.001223         0.146409\n",
            "              topic_concentration   Topic Coherence     0.000926         0.110822\n",
            "    mean_entity_continuation_rate   Entity Cohesion     0.000722         0.086420\n",
            "     adjacent_similarity_variance Semantic Cohesion    -0.000063        -0.007565\n",
            "            long_range_similarity Semantic Cohesion    -0.000163        -0.019453\n",
            "                entity_reuse_rate   Entity Cohesion    -0.000324        -0.038829\n",
            "      topic_transition_similarity   Topic Coherence    -0.000519        -0.062140\n",
            "                topic_persistence   Topic Coherence    -0.000532        -0.063631\n",
            "Bottom 10 Features (Avoid Adding):\n",
            "                      feature             group  improvement  improvement_pct\n",
            "            topic_persistence   Topic Coherence    -0.000532        -0.063631\n",
            "          num_distinct_topics   Topic Coherence    -0.000549        -0.065751\n",
            "    entity_isolated_sentences   Entity Cohesion    -0.000589        -0.070465\n",
            "    topic_switching_frequency   Topic Coherence    -0.000609        -0.072926\n",
            "       entity_mention_density   Entity Cohesion    -0.000637        -0.076294\n",
            "         entity_graph_density   Entity Cohesion    -0.000831        -0.099416\n",
            "entity_largest_component_size   Entity Cohesion    -0.001025        -0.122648\n",
            "             topic_drift_rate   Topic Coherence    -0.001170        -0.139994\n",
            "        similarity_decay_rate Semantic Cohesion    -0.001865        -0.223239\n",
            "            topic_return_rate   Topic Coherence    -0.003243        -0.388176\n",
            "Group-Level Performance (Individual Additions):\n",
            "                   Mean Δ   Std Δ   Min Δ   Max Δ  N Features\n",
            "group                                                        \n",
            "Entity Cohesion   -0.0004  0.0006 -0.0010  0.0007           6\n",
            "Semantic Cohesion  0.0021  0.0023 -0.0019  0.0048          10\n",
            "Topic Coherence   -0.0003  0.0019 -0.0032  0.0033           8\n",
            "Creating individual feature contribution visualization...\n",
            "  Saved: /content/drive/MyDrive/Tesi Magistrale/cohesion_analysis/plots/individual_feature_contributions.png\n",
            "Building optimal feature set progressively...\n",
            "\n",
            "  Step 0: Backbone Only\n",
            "    F1: 0.8355\n",
            "\n",
            "  Step 1: Testing 24 remaining features...\n",
            "    Best feature: mean_nonadjacent_similarity\n",
            "    F1: 0.8403 (±0.0071)\n",
            "    Improvement: +0.0048\n",
            "\n",
            "  Step 2: Testing 23 remaining features...\n",
            "    Best feature: adjacent_similarity_variance\n",
            "    F1: 0.8442 (±0.0082)\n",
            "    Improvement: +0.0038\n",
            "\n",
            "  Step 3: Testing 22 remaining features...\n",
            "    Best feature: entity_graph_density\n",
            "    F1: 0.8462 (±0.0044)\n",
            "    Improvement: +0.0020\n",
            "\n",
            "  Step 4: Testing 21 remaining features...\n",
            "    Best feature: semantic_average_degree\n",
            "    F1: 0.8496 (±0.0075)\n",
            "    Improvement: +0.0035\n",
            "\n",
            "  Step 5: Testing 20 remaining features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2532082731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         )\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FIXED: using y_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1806\u001b[0m             )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1809\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m             _check_call(\n\u001b[0;32m-> 2434\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2435\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}